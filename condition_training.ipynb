{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8293309,"sourceType":"datasetVersion","datasetId":4926875},{"sourceId":8294566,"sourceType":"datasetVersion","datasetId":4927464},{"sourceId":31184,"sourceType":"modelInstanceVersion","modelInstanceId":26158}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/eladrich/pixel2style2pixel.git","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:21:56.220136Z","iopub.execute_input":"2024-05-19T08:21:56.220810Z","iopub.status.idle":"2024-05-19T08:22:05.009197Z","shell.execute_reply.started":"2024-05-19T08:21:56.220777Z","shell.execute_reply":"2024-05-19T08:22:05.008240Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Cloning into 'pixel2style2pixel'...\nremote: Enumerating objects: 418, done.\u001b[K\nremote: Counting objects: 100% (4/4), done.\u001b[K\nremote: Compressing objects: 100% (4/4), done.\u001b[K\nremote: Total 418 (delta 0), reused 2 (delta 0), pack-reused 414\u001b[K\nReceiving objects: 100% (418/418), 92.94 MiB | 14.75 MiB/s, done.\nResolving deltas: 100% (147/147), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/pixel2style2pixel","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:22:05.011127Z","iopub.execute_input":"2024-05-19T08:22:05.011445Z","iopub.status.idle":"2024-05-19T08:22:05.018373Z","shell.execute_reply.started":"2024-05-19T08:22:05.011417Z","shell.execute_reply":"2024-05-19T08:22:05.017507Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/kaggle/working/pixel2style2pixel\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install gdown","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall -y datasets","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:22:05.019540Z","iopub.execute_input":"2024-05-19T08:22:05.019807Z","iopub.status.idle":"2024-05-19T08:22:06.893727Z","shell.execute_reply.started":"2024-05-19T08:22:05.019785Z","shell.execute_reply":"2024-05-19T08:22:06.892567Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping datasets as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:22:06.896088Z","iopub.execute_input":"2024-05-19T08:22:06.896402Z","iopub.status.idle":"2024-05-19T08:22:18.921796Z","shell.execute_reply.started":"2024-05-19T08:22:06.896375Z","shell.execute_reply":"2024-05-19T08:22:18.920605Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /opt/conda/lib/python3.10/site-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir pretrained_models","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:22:18.923330Z","iopub.execute_input":"2024-05-19T08:22:18.923663Z","iopub.status.idle":"2024-05-19T08:22:19.860418Z","shell.execute_reply.started":"2024-05-19T08:22:18.923634Z","shell.execute_reply":"2024-05-19T08:22:19.859302Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!gdown --id 1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT \n!mv /kaggle/working/pixel2style2pixel/stylegan2-ffhq-config-f.pt /kaggle/working/pixel2style2pixel/pretrained_models","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:22:19.863435Z","iopub.execute_input":"2024-05-19T08:22:19.864125Z","iopub.status.idle":"2024-05-19T08:22:39.658777Z","shell.execute_reply.started":"2024-05-19T08:22:19.864084Z","shell.execute_reply":"2024-05-19T08:22:39.657581Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT\nFrom (redirected): https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT&confirm=t&uuid=6e59b731-e2fe-4912-8b13-3a985697b31e\nTo: /kaggle/working/pixel2style2pixel/stylegan2-ffhq-config-f.pt\n100%|████████████████████████████████████████| 381M/381M [00:16<00:00, 23.8MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1KW7bjndL3QG3sxBbZxreGHigcCCpsDgn\n!mv /kaggle/working/pixel2style2pixel/model_ir_se50.pth /kaggle/working/pixel2style2pixel/pretrained_models","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:22:39.660491Z","iopub.execute_input":"2024-05-19T08:22:39.660905Z","iopub.status.idle":"2024-05-19T08:22:48.060687Z","shell.execute_reply.started":"2024-05-19T08:22:39.660868Z","shell.execute_reply":"2024-05-19T08:22:48.059527Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1KW7bjndL3QG3sxBbZxreGHigcCCpsDgn\nFrom (redirected): https://drive.google.com/uc?id=1KW7bjndL3QG3sxBbZxreGHigcCCpsDgn&confirm=t&uuid=f9fcb4ad-ad4f-4824-86e4-7855985584f7\nTo: /kaggle/working/pixel2style2pixel/model_ir_se50.pth\n100%|████████████████████████████████████████| 175M/175M [00:04<00:00, 40.2MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/pixel2style2pixel/configs/paths_config.py\ndataset_paths = {\n\t'celeba_train': '/kaggle/input/celebhq8020/celebahq256_all/train',\n\t'celeba_test': '/kaggle/input/celebhq8020/celebahq256_all/test',\n    'celeba_train_sketch': '',\n\t'celeba_test_sketch': '',\n\t'celeba_train_segmentation': '',\n\t'celeba_test_segmentation': '',\n\t'ffhq': '',\n}\n\nmodel_paths = {\n\t'stylegan_ffhq': 'pretrained_models/stylegan2-ffhq-config-f.pt',\n\t'ir_se50': 'pretrained_models/model_ir_se50.pth',\n\t'circular_face': 'pretrained_models/CurricularFace_Backbone.pth',\n\t'mtcnn_pnet': 'pretrained_models/mtcnn/pnet.npy',\n\t'mtcnn_rnet': 'pretrained_models/mtcnn/rnet.npy',\n\t'mtcnn_onet': 'pretrained_models/mtcnn/onet.npy',\n\t'shape_predictor': 'shape_predictor_68_face_landmarks.dat',\n\t'moco': 'pretrained_models/moco_v2_800ep_pretrain.pth.tar'\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:22:48.062154Z","iopub.execute_input":"2024-05-19T08:22:48.062492Z","iopub.status.idle":"2024-05-19T08:22:48.070068Z","shell.execute_reply.started":"2024-05-19T08:22:48.062463Z","shell.execute_reply":"2024-05-19T08:22:48.068937Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/pixel2style2pixel/configs/paths_config.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/pixel2style2pixel/configs/data_configs.py\nfrom configs import transforms_config\nfrom configs.paths_config import dataset_paths\n\n\nDATASETS = {\n\t'ffhq_encode': {\n\t\t'transforms': transforms_config.EncodeTransforms,\n\t\t'train_source_root': dataset_paths['celeba_train'],\n\t\t'train_target_root': dataset_paths['celeba_train'],\n\t\t'test_source_root': dataset_paths['celeba_test'],\n\t\t'test_target_root': dataset_paths['celeba_test'],\n\t},\n\t'ffhq_frontalize': {\n\t\t'transforms': transforms_config.FrontalizationTransforms,\n\t\t'train_source_root': dataset_paths['ffhq'],\n\t\t'train_target_root': dataset_paths['ffhq'],\n\t\t'test_source_root': dataset_paths['celeba_test'],\n\t\t'test_target_root': dataset_paths['celeba_test'],\n\t},\n\t'celebs_sketch_to_face': {\n\t\t'transforms': transforms_config.SketchToImageTransforms,\n\t\t'train_source_root': dataset_paths['celeba_train_sketch'],\n\t\t'train_target_root': dataset_paths['celeba_train'],\n\t\t'test_source_root': dataset_paths['celeba_test_sketch'],\n\t\t'test_target_root': dataset_paths['celeba_test'],\n\t},\n\t'celebs_seg_to_face': {\n\t\t'transforms': transforms_config.SegToImageTransforms,\n\t\t'train_source_root': dataset_paths['celeba_train_segmentation'],\n\t\t'train_target_root': dataset_paths['celeba_train'],\n\t\t'test_source_root': dataset_paths['celeba_test_segmentation'],\n\t\t'test_target_root': dataset_paths['celeba_test'],\n\t},\n\t'celebs_super_resolution': {\n\t\t'transforms': transforms_config.SuperResTransforms,\n\t\t'train_source_root': dataset_paths['celeba_train'],\n\t\t'train_target_root': dataset_paths['celeba_train'],\n\t\t'test_source_root': dataset_paths['celeba_test'],\n\t\t'test_target_root': dataset_paths['celeba_test'],\n\t},\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:22:48.071278Z","iopub.execute_input":"2024-05-19T08:22:48.071529Z","iopub.status.idle":"2024-05-19T08:22:48.086037Z","shell.execute_reply.started":"2024-05-19T08:22:48.071508Z","shell.execute_reply":"2024-05-19T08:22:48.085230Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/pixel2style2pixel/configs/data_configs.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/pixel2style2pixel/configs/transforms_config.py\nfrom abc import abstractmethod\nimport torchvision.transforms as transforms\nfrom pixel2style2pixel.datasets import augmentations\n\n\nclass TransformsConfig(object):\n\n\tdef __init__(self, opts):\n\t\tself.opts = opts\n\n\t@abstractmethod\n\tdef get_transforms(self):\n\t\tpass\n\n\nclass EncodeTransforms(TransformsConfig):\n\n\tdef __init__(self, opts):\n\t\tsuper(EncodeTransforms, self).__init__(opts)\n\n\tdef get_transforms(self):\n\t\ttransforms_dict = {\n\t\t\t'transform_gt_train': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_source': transforms.Compose([\n                transforms.Resize((256, 256)),\n                transforms.Grayscale(num_output_channels=3), # convert  examples to grayscale\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]), # convert  examples to grayscale\n\t\t\t'transform_test': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_inference': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n\t\t}\n\t\treturn transforms_dict\n\n\nclass FrontalizationTransforms(TransformsConfig):\n\n\tdef __init__(self, opts):\n\t\tsuper(FrontalizationTransforms, self).__init__(opts)\n\n\tdef get_transforms(self):\n\t\ttransforms_dict = {\n\t\t\t'transform_gt_train': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.RandomHorizontalFlip(0.5),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_source': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.RandomHorizontalFlip(0.5),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_test': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_inference': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n\t\t}\n\t\treturn transforms_dict\n\n\nclass SketchToImageTransforms(TransformsConfig):\n\n\tdef __init__(self, opts):\n\t\tsuper(SketchToImageTransforms, self).__init__(opts)\n\n\tdef get_transforms(self):\n\t\ttransforms_dict = {\n\t\t\t'transform_gt_train': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_source': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor()]),\n\t\t\t'transform_test': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_inference': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor()]),\n\t\t}\n\t\treturn transforms_dict\n\n\nclass SegToImageTransforms(TransformsConfig):\n\n\tdef __init__(self, opts):\n\t\tsuper(SegToImageTransforms, self).__init__(opts)\n\n\tdef get_transforms(self):\n\t\ttransforms_dict = {\n\t\t\t'transform_gt_train': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_source': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\taugmentations.ToOneHot(self.opts.label_nc),\n\t\t\t\ttransforms.ToTensor()]),\n\t\t\t'transform_test': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_inference': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\taugmentations.ToOneHot(self.opts.label_nc),\n\t\t\t\ttransforms.ToTensor()])\n\t\t}\n\t\treturn transforms_dict\n\n\nclass SuperResTransforms(TransformsConfig):\n\n\tdef __init__(self, opts):\n\t\tsuper(SuperResTransforms, self).__init__(opts)\n\n\tdef get_transforms(self):\n\t\tif self.opts.resize_factors is None:\n\t\t\tself.opts.resize_factors = '1,2,4,8,16,32'\n\t\tfactors = [int(f) for f in self.opts.resize_factors.split(\",\")]\n\t\tprint(\"Performing down-sampling with factors: {}\".format(factors))\n\t\ttransforms_dict = {\n\t\t\t'transform_gt_train': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_source': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\taugmentations.BilinearResize(factors=factors),\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_test': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_inference': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\taugmentations.BilinearResize(factors=factors),\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n\t\t}\n\t\treturn transforms_dict\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:22:48.087327Z","iopub.execute_input":"2024-05-19T08:22:48.087665Z","iopub.status.idle":"2024-05-19T08:22:48.101832Z","shell.execute_reply.started":"2024-05-19T08:22:48.087641Z","shell.execute_reply":"2024-05-19T08:22:48.100886Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/pixel2style2pixel/configs/transforms_config.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/pixel2style2pixel/models/psp.py\n\"\"\"\nThis file defines the core research contribution\n\"\"\"\nimport matplotlib\nmatplotlib.use('Agg')\nimport math\n\nimport torch\nfrom torch import nn\nfrom models.encoders import psp_encoders\nfrom models.stylegan2.model import Generator\nfrom configs.paths_config import model_paths\n\n\ndef get_keys(d, name):\n\tif 'state_dict' in d:\n\t\td = d['state_dict']\n\td_filt = {k[len(name) + 1:]: v for k, v in d.items() if k[:len(name)] == name}\n\treturn d_filt\n\n\nclass pSp(nn.Module):\n\n\tdef __init__(self, opts):\n\t\tsuper(pSp, self).__init__()\n\t\tself.set_opts(opts)\n\t\t# compute number of style inputs based on the output resolution\n\t\tself.opts.n_styles = int(math.log(self.opts.output_size, 2)) * 2 - 2\n\t\t# Define architecture\n\t\tself.encoder = self.set_encoder()\n\t\tself.decoder = Generator(self.opts.output_size, 512, 8)\n\t\tself.face_pool = torch.nn.AdaptiveAvgPool2d((256, 256))\n\t\t# Load weights if needed\n\t\tself.load_weights()\n\n\tdef set_encoder(self):\n\t\tif self.opts.encoder_type == 'GradualStyleEncoder':\n\t\t\tencoder = psp_encoders.GradualStyleEncoder(50, 'ir_se', self.opts)\n\t\telif self.opts.encoder_type == 'BackboneEncoderUsingLastLayerIntoW':\n\t\t\tencoder = psp_encoders.BackboneEncoderUsingLastLayerIntoW(50, 'ir_se', self.opts)\n\t\telif self.opts.encoder_type == 'BackboneEncoderUsingLastLayerIntoWPlus':\n\t\t\tencoder = psp_encoders.BackboneEncoderUsingLastLayerIntoWPlus(50, 'ir_se', self.opts)\n\t\telse:\n\t\t\traise Exception('{} is not a valid encoders'.format(self.opts.encoder_type))\n\t\treturn encoder\n\n\tdef load_weights(self):\n\t\tif self.opts.checkpoint_path is not None:\n\t\t\tprint('Loading pSp from checkpoint: {}'.format(self.opts.checkpoint_path))\n\t\t\tckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n\t\t\tself.encoder.load_state_dict(get_keys(ckpt, 'encoder'), strict=True)\n\t\t\tself.decoder.load_state_dict(get_keys(ckpt, 'decoder'), strict=True)\n\t\t\tself.__load_latent_avg(ckpt)\n\t\telse:\n\t\t\tprint('Loading encoders weights from irse50!')\n\t\t\tencoder_ckpt = torch.load(model_paths['ir_se50'])\n\t\t\t# if input to encoder is not an RGB image, do not load the input layer weights\n\t\t\tif self.opts.label_nc != 0:\n\t\t\t\tencoder_ckpt = {k: v for k, v in encoder_ckpt.items() if \"input_layer\" not in k}\n\t\t\tself.encoder.load_state_dict(encoder_ckpt, strict=False)\n\t\t\tprint('Loading decoder weights from pretrained!')\n\t\t\tckpt = torch.load(self.opts.stylegan_weights)\n\t\t\tself.decoder.load_state_dict(ckpt['g_ema'], strict=False)\n\t\t\tif self.opts.learn_in_w:\n\t\t\t\tself.__load_latent_avg(ckpt, repeat=1)\n\t\t\telse:\n\t\t\t\tself.__load_latent_avg(ckpt, repeat=self.opts.n_styles)\n\n\tdef forward(self, x, y_hist, resize=True, latent_mask=None, input_code=False, randomize_noise=True,\n\t            inject_latent=None, return_latents=False, alpha=None):\n\t\tif input_code:\n\t\t\tcodes = x\n\t\telse:\n            \n\t\t\tcodes = self.encoder(x, y_hist)\n\t\t\t# normalize with respect to the center of an average face\n\t\t\tif self.opts.start_from_latent_avg:\n\t\t\t\tif self.opts.learn_in_w:\n\t\t\t\t\tcodes = codes + self.latent_avg.repeat(codes.shape[0], 1)\n\t\t\t\telse:\n\t\t\t\t\tcodes = codes + self.latent_avg.repeat(codes.shape[0], 1, 1)\n\n\n\t\tif latent_mask is not None:\n\t\t\tfor i in latent_mask:\n\t\t\t\tif inject_latent is not None:\n\t\t\t\t\tif alpha is not None:\n\t\t\t\t\t\tcodes[:, i] = alpha * inject_latent[:, i] + (1 - alpha) * codes[:, i]\n\t\t\t\t\telse:\n\t\t\t\t\t\tcodes[:, i] = inject_latent[:, i]\n\t\t\t\telse:\n\t\t\t\t\tcodes[:, i] = 0\n\n\t\tinput_is_latent = not input_code\n\t\timages, result_latent = self.decoder([codes],\n\t\t                                     input_is_latent=input_is_latent,\n\t\t                                     randomize_noise=randomize_noise,\n\t\t                                     return_latents=return_latents)\n\n\t\tif resize:\n\t\t\timages = self.face_pool(images)\n\n\t\tif return_latents:\n\t\t\treturn images, result_latent\n\t\telse:\n\t\t\treturn images\n\n\tdef set_opts(self, opts):\n\t\tself.opts = opts\n\n\tdef __load_latent_avg(self, ckpt, repeat=None):\n\t\tif 'latent_avg' in ckpt:\n\t\t\tself.latent_avg = ckpt['latent_avg'].to(self.opts.device)\n\t\t\tif repeat is not None:\n\t\t\t\tself.latent_avg = self.latent_avg.repeat(repeat, 1)\n\t\telse:\n\t\t\tself.latent_avg = None\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:22:48.105419Z","iopub.execute_input":"2024-05-19T08:22:48.105694Z","iopub.status.idle":"2024-05-19T08:22:48.117253Z","shell.execute_reply.started":"2024-05-19T08:22:48.105671Z","shell.execute_reply":"2024-05-19T08:22:48.116328Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/pixel2style2pixel/models/psp.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/pixel2style2pixel/training/histogram.py\n\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn.functional as F\n\ndef batch_histogram_feature(img, epsilon=1e-6, tau=0.3, num_bins=64):\n    R, G, B = image[:, 0, :, :], image[:, 1, :, :], image[:, 2, :, :]\n\n    # Compute log-chroma for each channel\n    log_chroma_RG = torch.log((R + epsilon) / (G + epsilon))\n    log_chroma_RB = torch.log((R + epsilon) / (B + epsilon))\n    log_chroma_GB = torch.log((G + epsilon) / (B + epsilon))\n\n    # Compute intensity for each pixel\n    intensity = torch.sqrt(R**2 + G**2 + B**2)\n\n    # Inverse-quadratic kernel\n    def kernel(log_chroma_u, log_chroma_v, u, v):\n        return ((1 + ((torch.abs(log_chroma_u - u)) / tau)**2)**-1) * ((1 + ((torch.abs(log_chroma_v - v)) / tau)**2)**-1)\n\n    # Compute unnormalized histograms\n    histograms = torch.zeros((image.size(0), 3, num_bins, num_bins), device=image.device)\n\n    bin_edges = torch.linspace(log_chroma_RG.min().item(), log_chroma_RG.max().item(), num_bins + 1, device=image.device)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    for u_idx in range(num_bins):\n        for v_idx in range(num_bins):\n            histograms[:, 0, u_idx, v_idx] = torch.sum(kernel(log_chroma_RG, log_chroma_RB, bin_centers[u_idx], bin_centers[v_idx]) * intensity, dim=[1, 2])\n            histograms[:, 1, u_idx, v_idx] = torch.sum(kernel(log_chroma_GB, log_chroma_RG, bin_centers[u_idx], bin_centers[v_idx]) * intensity, dim=[1, 2])\n            histograms[:, 2, u_idx, v_idx] = torch.sum(kernel(log_chroma_RB, log_chroma_GB, bin_centers[u_idx], bin_centers[v_idx]) * intensity, dim=[1, 2])\n\n    # Normalize histograms \n    histograms /= histograms.sum(dim=[2, 3], keepdim=True) \n\n    return histograms\n\ndef compute_histogram_pt(image, bins=256, min=0.0, max=1.0):\n    hist = torch.histc(image, bins=bins, min=min, max=max)\n    hist = hist / torch.sum(hist)  # Normalize the histogram\n    return hist\n\n\ndef compute_histogram_feature(image, epsilon=0.1, tau=0.3, num_bins=16):\n    # Convert image to float and extract RGB channels\n    image = image.astype(np.float32) / 255.0\n    R, G, B = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n\n    # Compute log-chroma space for each channel\n    chroma_R = R + epsilon\n    chroma_G = G + epsilon\n    chroma_B = B + epsilon\n\n\n    # Compute intensity for each pixel\n    intensity = np.sqrt(chroma_R**2 + chroma_G**2 + chroma_B**2)\n\n    # Define inverse-quadratic kernel\n    def kernel(I_uc, I_vc, u, v):\n        return ((1 + ((np.abs(I_uc - u)) / tau)**2)**-1) * ((1 + ((np.abs(I_vc - v)) / tau)**2)**-1)\n\n    # Compute unnormalized histograms for each channel\n    histograms = np.zeros((3, num_bins, num_bins))\n    for c_idx, (chroma) in enumerate([chroma_R, chroma_G, chroma_B]):\n        for u_idx in range(num_bins):\n            for v_idx in range(num_bins):\n                # normalize log_chroma before passing kernel with other 2 channels and obtain log_chroma_u and long_chroma_v\n                # then pass them to the kernel\n                if c_idx == 0:\n                    log_chroma_u = chroma / chroma_G\n                    log_chroma_v = chroma / chroma_B\n                elif c_idx == 1:\n                    log_chroma_u = chroma / chroma_R\n                    log_chroma_v = chroma / chroma_B\n                else:\n                    log_chroma_u = chroma / chroma_R\n                    log_chroma_v = chroma / chroma_G\n\n                log_chroma_u = np.log(log_chroma_u)\n                log_chroma_v = np.log(log_chroma_v)\n\n                hist_value = np.sum(kernel(log_chroma_u, log_chroma_v, u_idx, v_idx) * intensity)\n                histograms[c_idx, u_idx, v_idx] = hist_value\n\n    # Normalize histogram feature\n    total_intensity = np.sum(histograms, axis=(1, 2))  # Sum of intensities across all bins\n    histograms /= total_intensity[:, None, None]  # Normalize each channel by the total intensity\n\n    return histograms\n\ndef hist(image, N=64):\n    # Split the image into its RGB components\n    b = image[0, :, :]  # Blue channel\n    g = image[1, :, :]  # Green channel\n    r = image[2, :, :]  # Red channel\n    \n    # Calculate histograms\n    hist_r = cv2.calcHist([r], [0], None, [256], [0, 256])\n    hist_g = cv2.calcHist([g], [0], None, [256], [0, 256])\n    hist_b = cv2.calcHist([b], [0], None, [256], [0, 256])\n    \n    # Flatten histograms and convert to a torch tensor\n    histogram = np.array([hist_r, hist_g, hist_b])\n    histogram = torch.from_numpy(histogram)\n    \n    return histogram\n\ndef compute_histogram(image, N=64):\n    # Split the image into its RGB components\n    b = image[0, :, :]  # Blue channel\n    g = image[1, :, :]  # Green channel\n    r = image[2, :, :]  # Red channel\n    \n    # Calculate histograms\n    hist_r = cv2.calcHist([r], [0], None, [256], [0, 256])\n    hist_g = cv2.calcHist([g], [0], None, [256], [0, 256])\n    hist_b = cv2.calcHist([b], [0], None, [256], [0, 256])\n    \n    # Flatten histograms and convert to a torch tensor\n    histogram = np.array([hist_r, hist_g, hist_b])\n    histogram = torch.from_numpy(histogram)\n    \n    hist = histogram.unsqueeze(0).unsqueeze(0)\n    \n    # Resize histogram to (256, 64, 64)\n    histogram_resized = F.interpolate(hist, size=(256, 64, 64), mode='nearest')  # Shape: (1, 256, 64, 64)\n    \n    # Squeeze and return\n    return histogram_resized.squeeze().squeeze()\n\ndef compute_2d_histogram(condition_img):\n    hist_list = []\n    bins = 16\n    tick_spacing = 5\n    channels_mapping = {0: 'B', 1: 'G', 2: 'R'}\n    for i, channels in enumerate([[0, 1], [0, 2], [1, 2]]):\n        hist = cv2.calcHist([condition_img], channels, None, [bins] * 2, [0, 256] * 2)\n        hist_list.append(hist)\n        \n    condition_hist = np.array(hist_list)\n    condition_hist = torch.from_numpy(condition_hist)\n    return condition_hist","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:47:16.744007Z","iopub.execute_input":"2024-05-19T08:47:16.744480Z","iopub.status.idle":"2024-05-19T08:47:16.755991Z","shell.execute_reply.started":"2024-05-19T08:47:16.744435Z","shell.execute_reply":"2024-05-19T08:47:16.754968Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/pixel2style2pixel/training/histogram.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/pixel2style2pixel/training/coach.py\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nmatplotlib.use('Agg')\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.functional as F\n\nfrom utils import common, train_utils\nfrom criteria import id_loss, w_norm, moco_loss\nfrom configs import data_configs\nfrom datasets.images_dataset import ImagesDataset\nfrom criteria.lpips.lpips import LPIPS\nfrom models.psp import pSp\nfrom training.ranger import Ranger\nimport cv2\nimport numpy as np\nfrom training.histogram import compute_histogram, compute_histogram_pt\n\nclass Coach:\n\tdef __init__(self, opts):\n\t\tself.opts = opts\n\n\t\tself.global_step = 0\n\n\t\tself.device = 'cuda:0'  # TODO: Allow multiple GPU? currently using CUDA_VISIBLE_DEVICES\n\t\tself.opts.device = self.device\n\t\tself.hist_lambda = 0.5\n\n\t\tif self.opts.use_wandb:\n\t\t\tfrom utils.wandb_utils import WBLogger\n\t\t\tself.wb_logger = WBLogger(self.opts)\n\n\t\t# Initialize network\n\t\tself.net = pSp(self.opts).to(self.device)\n\n\t\t# Estimate latent_avg via dense sampling if latent_avg is not available\n\t\tif self.net.latent_avg is None:\n\t\t\tself.net.latent_avg = self.net.decoder.mean_latent(int(1e5))[0].detach()\n\n\t\t# Initialize loss\n\t\tif self.opts.id_lambda > 0 and self.opts.moco_lambda > 0:\n\t\t\traise ValueError('Both ID and MoCo loss have lambdas > 0! Please select only one to have non-zero lambda!')\n\n\t\tself.mse_loss = nn.MSELoss().to(self.device).eval()\n\t\tif self.opts.lpips_lambda > 0:\n\t\t\tself.lpips_loss = LPIPS(net_type='alex').to(self.device).eval()\n\t\tif self.opts.id_lambda > 0:\n\t\t\tself.id_loss = id_loss.IDLoss().to(self.device).eval()\n\t\tif self.opts.w_norm_lambda > 0:\n\t\t\tself.w_norm_loss = w_norm.WNormLoss(start_from_latent_avg=self.opts.start_from_latent_avg)\n\t\tif self.opts.moco_lambda > 0:\n\t\t\tself.moco_loss = moco_loss.MocoLoss().to(self.device).eval()\n\n\t\t# Initialize optimizer\n\t\tself.optimizer = self.configure_optimizers()\n\n\t\t# Initialize dataset\n\t\tself.train_dataset, self.test_dataset = self.configure_datasets()\n\t\tself.train_dataloader = DataLoader(self.train_dataset,\n\t\t\t\t\t\t\t\t\t\t   batch_size=self.opts.batch_size,\n\t\t\t\t\t\t\t\t\t\t   shuffle=True,\n\t\t\t\t\t\t\t\t\t\t   num_workers=int(self.opts.workers),\n\t\t\t\t\t\t\t\t\t\t   drop_last=True)\n\t\tself.test_dataloader = DataLoader(self.test_dataset,\n\t\t\t\t\t\t\t\t\t\t  batch_size=self.opts.test_batch_size,\n\t\t\t\t\t\t\t\t\t\t  shuffle=False,\n\t\t\t\t\t\t\t\t\t\t  num_workers=int(self.opts.test_workers),\n\t\t\t\t\t\t\t\t\t\t  drop_last=True)\n\n\t\t# Initialize logger\n\t\tlog_dir = os.path.join(opts.exp_dir, 'logs')\n\t\tos.makedirs(log_dir, exist_ok=True)\n\t\tself.logger = SummaryWriter(log_dir=log_dir)\n\n\t\t# Initialize checkpoint dir\n\t\tself.checkpoint_dir = os.path.join(opts.exp_dir, 'checkpoints')\n\t\tos.makedirs(self.checkpoint_dir, exist_ok=True)\n\t\tself.best_val_loss = None\n\t\tif self.opts.save_interval is None:\n\t\t\tself.opts.save_interval = self.opts.max_steps\n\n    \n\tdef train(self):\n\t\tbins = 16\n\t\tself.net.train()\n\t\twhile self.global_step < self.opts.max_steps:\n\t\t\tfor batch_idx, batch in enumerate(self.train_dataloader):\n\t\t\t\tself.optimizer.zero_grad()\n\t\t\t\tx, y = batch\n\t\t\t\t#print(x.shape)\n\t\t\t\ty_np = y.numpy()\n\t\t\t\thist_list_y = []\n\t\t\t\tfor image in y_np:\n\t\t\t\t\thist = compute_histogram(image)\n\t\t\t\t\t#print(hist.shape)\n\t\t\t\t\thist_list_y.append(hist)\n\n\t\t\t\ty_hist = np.array(hist_list_y)\n\t\t\t\tx, y = x.to(self.device).float(), y.to(self.device).float()\n\t\t\t\ty_hist = torch.from_numpy(y_hist).to(self.device).float()\n            \n\t\t\t\ty_hat, latent = self.net.forward(x, y_hist, return_latents=True)\n\t\t\t\tif self.global_step % 5001 == 0:\n\t\t\t\t\tself.hist_lambda *= 2       \n\t\t\t\tloss, loss_dict, id_logs = self.calc_loss(x, y, y_hat, latent)\n\t\t\t\tloss.backward()\n\t\t\t\tself.optimizer.step()\n\n\t\t\t\t# Logging related\n\t\t\t\tif self.global_step % self.opts.image_interval == 0 or (self.global_step < 1000 and self.global_step % 25 == 0):\n\t\t\t\t\tself.parse_and_log_images(id_logs, x, y, y_hat, title='images/train/faces')\n\t\t\t\tif self.global_step % self.opts.board_interval == 0:\n\t\t\t\t\tself.print_metrics(loss_dict, prefix='train')\n\t\t\t\t\tself.log_metrics(loss_dict, prefix='train')\n\n\t\t\t\t# Log images of first batch to wandb\n\t\t\t\tif self.opts.use_wandb and batch_idx == 0:\n\t\t\t\t\tself.wb_logger.log_images_to_wandb(x, y, y_hat, id_logs, prefix=\"train\", step=self.global_step, opts=self.opts)\n\n\t\t\t\t# Validation related\n\t\t\t\tval_loss_dict = None\n\t\t\t\tif self.global_step % self.opts.val_interval == 0 or self.global_step == self.opts.max_steps:\n\t\t\t\t\tval_loss_dict = self.validate()\n\t\t\t\t\tif val_loss_dict and (self.best_val_loss is None or val_loss_dict['loss'] < self.best_val_loss):\n\t\t\t\t\t\tself.best_val_loss = val_loss_dict['loss']\n\t\t\t\t\t\tself.checkpoint_me(val_loss_dict, is_best=True)\n\n\t\t\t\tif self.global_step % self.opts.save_interval == 0 or self.global_step == self.opts.max_steps:\n\t\t\t\t\tif val_loss_dict is not None:\n\t\t\t\t\t\tself.checkpoint_me(val_loss_dict, is_best=False)\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.checkpoint_me(loss_dict, is_best=False)\n\n\t\t\t\tif self.global_step == self.opts.max_steps:\n\t\t\t\t\tprint('OMG, finished training!')\n\t\t\t\t\tbreak\n\n\t\t\t\tself.global_step += 1\n\n\tdef validate(self):\n\t\tbins=16\n\t\tself.net.eval()\n\t\tagg_loss_dict = []\n\t\tfor batch_idx, batch in enumerate(self.test_dataloader):\n\t\t\tx, y = batch\n\t\t\ty_np = y.numpy()\n\t\t\thist_list_y = []\n\t\t\tfor image in y_np:\n\t\t\t\thist = compute_histogram(image)\n\t\t\t\thist_list_y.append(hist)\n\n\t\t\ty_hist = np.array(hist_list_y)\n\n\t\t\twith torch.no_grad():\n\t\t\t\ty_hist = torch.from_numpy(y_hist).to(self.device).float()\n\t\t\t\tx, y = x.to(self.device).float(), y.to(self.device).float()\n\t\t\t\ty_hat, latent = self.net.forward(x, y_hist, return_latents=True)\n\t\t\t\tloss, cur_loss_dict, id_logs = self.calc_loss(x, y, y_hat, latent)\n\t\t\tagg_loss_dict.append(cur_loss_dict)\n\n\t\t\t# Logging related\n\t\t\tself.parse_and_log_images(id_logs, x, y, y_hat,\n\t\t\t\t\t\t\t\t\t  title='images/test/faces',\n\t\t\t\t\t\t\t\t\t  subscript='{:04d}'.format(batch_idx))\n\n\t\t\t# Log images of first batch to wandb\n\t\t\tif self.opts.use_wandb and batch_idx == 0:\n\t\t\t\tself.wb_logger.log_images_to_wandb(x, y, y_hat, id_logs, prefix=\"test\", step=self.global_step, opts=self.opts)\n\n\t\t\t# For first step just do sanity test on small amount of data\n\t\t\tif self.global_step == 0 and batch_idx >= 4:\n\t\t\t\tself.net.train()\n\t\t\t\treturn None  # Do not log, inaccurate in first batch\n\n\t\tloss_dict = train_utils.aggregate_loss_dict(agg_loss_dict)\n\t\tself.log_metrics(loss_dict, prefix='test')\n\t\tself.print_metrics(loss_dict, prefix='test')\n\n\t\tself.net.train()\n\t\treturn loss_dict\n\n\tdef checkpoint_me(self, loss_dict, is_best):\n\t\tsave_name = 'best_model.pt' if is_best else f'iteration_{self.global_step}.pt'\n\t\tsave_dict = self.__get_save_dict()\n\t\tcheckpoint_path = os.path.join(self.checkpoint_dir, save_name)\n\t\ttorch.save(save_dict, checkpoint_path)\n\t\twith open(os.path.join(self.checkpoint_dir, 'timestamp.txt'), 'a') as f:\n\t\t\tif is_best:\n\t\t\t\tf.write(f'**Best**: Step - {self.global_step}, Loss - {self.best_val_loss} \\n{loss_dict}\\n')\n\t\t\t\tif self.opts.use_wandb:\n\t\t\t\t\tself.wb_logger.log_best_model()\n\t\t\telse:\n\t\t\t\tf.write(f'Step - {self.global_step}, \\n{loss_dict}\\n')\n\n\tdef configure_optimizers(self):\n\t\tparams = list(self.net.encoder.parameters())\n\t\tif self.opts.train_decoder:\n\t\t\tparams += list(self.net.decoder.parameters())\n\t\tif self.opts.optim_name == 'adam':\n\t\t\toptimizer = torch.optim.Adam(params, lr=self.opts.learning_rate)\n\t\telse:\n\t\t\toptimizer = Ranger(params, lr=self.opts.learning_rate)\n\t\treturn optimizer\n\n\tdef configure_datasets(self):\n\t\tif self.opts.dataset_type not in data_configs.DATASETS.keys():\n\t\t\tException(f'{self.opts.dataset_type} is not a valid dataset_type')\n\t\tprint(f'Loading dataset for {self.opts.dataset_type}')\n\t\tdataset_args = data_configs.DATASETS[self.opts.dataset_type]\n\t\ttransforms_dict = dataset_args['transforms'](self.opts).get_transforms()\n\t\ttrain_dataset = ImagesDataset(source_root=dataset_args['train_source_root'],\n\t\t\t\t\t\t\t\t\t  target_root=dataset_args['train_target_root'],\n\t\t\t\t\t\t\t\t\t  source_transform=transforms_dict['transform_source'],\n\t\t\t\t\t\t\t\t\t  target_transform=transforms_dict['transform_gt_train'],\n\t\t\t\t\t\t\t\t\t  opts=self.opts)\n\t\ttest_dataset = ImagesDataset(source_root=dataset_args['test_source_root'],\n\t\t\t\t\t\t\t\t\t target_root=dataset_args['test_target_root'],\n\t\t\t\t\t\t\t\t\t source_transform=transforms_dict['transform_source'],\n\t\t\t\t\t\t\t\t\t target_transform=transforms_dict['transform_test'],\n\t\t\t\t\t\t\t\t\t opts=self.opts)\n\t\tif self.opts.use_wandb:\n\t\t\tself.wb_logger.log_dataset_wandb(train_dataset, dataset_name=\"Train\")\n\t\t\tself.wb_logger.log_dataset_wandb(test_dataset, dataset_name=\"Test\")\n\t\tprint(f\"Number of training samples: {len(train_dataset)}\")\n\t\tprint(f\"Number of test samples: {len(test_dataset)}\")\n\t\treturn train_dataset, test_dataset\n\n\tdef calc_loss(self, x, y, y_hat, latent):\n\t\tloss_dict = {}\n\t\tloss = 0.0\n\t\tid_logs = None\n\t\tif self.opts.id_lambda > 0:\n\t\t\tloss_id, sim_improvement, id_logs = self.id_loss(y_hat, y, x)\n\t\t\tloss_dict['loss_id'] = float(loss_id)\n\t\t\tloss_dict['id_improve'] = float(sim_improvement)\n\t\t\tloss = loss_id * self.opts.id_lambda\n\t\tif self.opts.l2_lambda > 0:\n\t\t\tloss_l2 = F.mse_loss(y_hat, y)\n\t\t\tloss_dict['loss_l2'] = float(loss_l2)\n\t\t\tloss += loss_l2 * self.opts.l2_lambda\n\t\tif self.opts.lpips_lambda > 0:\n\t\t\tloss_lpips = self.lpips_loss(y_hat, y)\n\t\t\tloss_dict['loss_lpips'] = float(loss_lpips)\n\t\t\tloss += loss_lpips * self.opts.lpips_lambda\n\t\tif self.opts.lpips_lambda_crop > 0:\n\t\t\tloss_lpips_crop = self.lpips_loss(y_hat[:, :, 35:223, 32:220], y[:, :, 35:223, 32:220])\n\t\t\tloss_dict['loss_lpips_crop'] = float(loss_lpips_crop)\n\t\t\tloss += loss_lpips_crop * self.opts.lpips_lambda_crop\n\t\tif self.opts.l2_lambda_crop > 0:\n\t\t\tloss_l2_crop = F.mse_loss(y_hat[:, :, 35:223, 32:220], y[:, :, 35:223, 32:220])\n\t\t\tloss_dict['loss_l2_crop'] = float(loss_l2_crop)\n\t\t\tloss += loss_l2_crop * self.opts.l2_lambda_crop\n\t\tif self.opts.w_norm_lambda > 0:\n\t\t\tloss_w_norm = self.w_norm_loss(latent, self.net.latent_avg)\n\t\t\tloss_dict['loss_w_norm'] = float(loss_w_norm)\n\t\t\tloss += loss_w_norm * self.opts.w_norm_lambda\n\t\tif self.opts.moco_lambda > 0:\n\t\t\tloss_moco, sim_improvement, id_logs = self.moco_loss(y_hat, y, x)\n\t\t\tloss_dict['loss_moco'] = float(loss_moco)\n\t\t\tloss_dict['id_improve'] = float(sim_improvement)\n\t\t\tloss += loss_moco * self.opts.moco_lambda\n\t\tloss_hist = 0\n\t\tfor i, image in enumerate(y):\n\t\t\t#print(image.shape)\n\t\t\t#print(y_hat[i].shape)\n\t\t\tabs_a = torch.abs(compute_histogram_pt(image) - compute_histogram_pt(y_hat[i]))\n\t\t\t#print(abs_a.shape)\n\t\t\tsum_a = torch.mean(abs_a)\n\t\t\t#print(sum_a)\n\t\t\tloss_hist += sum_a\n\t\tloss_hist = float(loss_hist / len(y))\n\t\tloss_dict[\"hist_loss\"] = loss_hist\n\t\tloss += loss_hist * self.hist_lambda\n\t\tloss_dict['loss'] = float(loss)\n\t\treturn loss, loss_dict, id_logs\n\n\tdef log_metrics(self, metrics_dict, prefix):\n\t\tfor key, value in metrics_dict.items():\n\t\t\tself.logger.add_scalar(f'{prefix}/{key}', value, self.global_step)\n\t\tif self.opts.use_wandb:\n\t\t\tself.wb_logger.log(prefix, metrics_dict, self.global_step)\n\n\tdef print_metrics(self, metrics_dict, prefix):\n\t\tprint(f'Metrics for {prefix}, step {self.global_step}')\n\t\tfor key, value in metrics_dict.items():\n\t\t\tprint(f'\\t{key} = ', value)\n\n\tdef parse_and_log_images(self, id_logs, x, y, y_hat, title, subscript=None, display_count=2):\n\t\tim_data = []\n\t\tfor i in range(display_count):\n\t\t\tcur_im_data = {\n\t\t\t\t'input_face': common.log_input_image(x[i], self.opts),\n\t\t\t\t'target_face': common.tensor2im(y[i]),\n\t\t\t\t'output_face': common.tensor2im(y_hat[i]),\n\t\t\t}\n\t\t\tif id_logs is not None:\n\t\t\t\tfor key in id_logs[i]:\n\t\t\t\t\tcur_im_data[key] = id_logs[i][key]\n\t\t\tim_data.append(cur_im_data)\n\t\tself.log_images(title, im_data=im_data, subscript=subscript)\n\n\tdef log_images(self, name, im_data, subscript=None, log_latest=False):\n\t\tfig = common.vis_faces(im_data)\n\t\tstep = self.global_step\n\t\tif log_latest:\n\t\t\tstep = 0\n\t\tif subscript:\n\t\t\tpath = os.path.join(self.logger.log_dir, name, f'{subscript}_{step:04d}.jpg')\n\t\telse:\n\t\t\tpath = os.path.join(self.logger.log_dir, name, f'{step:04d}.jpg')\n\t\tos.makedirs(os.path.dirname(path), exist_ok=True)\n\t\tfig.savefig(path)\n\t\tplt.close(fig)\n\n\tdef __get_save_dict(self):\n\t\tsave_dict = {\n\t\t\t'state_dict': self.net.state_dict(),\n\t\t\t'opts': vars(self.opts)\n\t\t}\n\t\t# save the latent avg in state_dict for inference if truncation of w was used during training\n\t\tif self.opts.start_from_latent_avg:\n\t\t\tsave_dict['latent_avg'] = self.net.latent_avg\n\t\treturn save_dict\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:52:10.068181Z","iopub.execute_input":"2024-05-19T08:52:10.068503Z","iopub.status.idle":"2024-05-19T08:52:10.084423Z","shell.execute_reply.started":"2024-05-19T08:52:10.068474Z","shell.execute_reply":"2024-05-19T08:52:10.083313Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/pixel2style2pixel/training/coach.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/pixel2style2pixel/models/encoders/psp_encoders.py\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn import Linear, Conv2d, BatchNorm2d, PReLU, Sequential, Module\n\nfrom models.encoders.helpers import get_blocks, Flatten, bottleneck_IR, bottleneck_IR_SE\nfrom models.stylegan2.model import EqualLinear\n\nimport torch.nn as nn\n\nclass TransposeConvNet(nn.Module):\n    def __init__(self):\n        super(TransposeConvNet, self).__init__()\n        \n        # Transpose convolution layers\n\n        self.conv1 = nn.ConvTranspose2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1)\n        self.conv2 = nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n        self.conv3 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n        \n        # Batch normalization layers\n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(128)\n        \n        # ReLU activation function\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        # Forward pass through the network\n        \n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.relu(self.bn2(self.conv2(x)))\n        x = self.conv3(x)\n        return x\n\nclass GradualStyleBlock(Module):\n    def __init__(self, in_c, out_c, spatial):\n        super(GradualStyleBlock, self).__init__()\n        self.out_c = out_c\n        self.spatial = spatial\n        num_pools = int(np.log2(spatial))\n        modules = []\n        modules += [Conv2d(in_c, out_c, kernel_size=3, stride=2, padding=1),\n                    nn.LeakyReLU()]\n        for i in range(num_pools - 1):\n            modules += [\n                Conv2d(out_c, out_c, kernel_size=3, stride=2, padding=1),\n                nn.LeakyReLU()\n            ]\n        self.convs = nn.Sequential(*modules)\n        self.linear = EqualLinear(out_c, out_c, lr_mul=1)\n\n    def forward(self, x):\n        x = self.convs(x)\n        x = x.view(-1, self.out_c)\n        x = self.linear(x)\n        return x\n\n\nclass GradualStyleEncoder(Module):\n    def __init__(self, num_layers, mode='ir', opts=None):\n        super(GradualStyleEncoder, self).__init__()\n        assert num_layers in [50, 100, 152], 'num_layers should be 50,100, or 152'\n        assert mode in ['ir', 'ir_se'], 'mode should be ir or ir_se'\n        blocks = get_blocks(num_layers)\n        if mode == 'ir':\n            unit_module = bottleneck_IR\n        elif mode == 'ir_se':\n            unit_module = bottleneck_IR_SE\n        self.condition_layer = TransposeConvNet()\n        \n        \n        \n        self.cat_size = 768\n        self.out_size = 512\n        \n        self.conv = nn.Conv2d(self.cat_size, self.out_size, kernel_size=1)\n        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n                                      BatchNorm2d(64),\n                                      PReLU(64))\n        modules = []\n        for block in blocks:\n            for bottleneck in block:\n                modules.append(unit_module(bottleneck.in_channel,\n                                           bottleneck.depth,\n                                           bottleneck.stride))\n        self.body = Sequential(*modules)\n\n        self.styles = nn.ModuleList()\n        self.style_count = opts.n_styles\n        self.coarse_ind = 3\n        self.middle_ind = 7\n        for i in range(self.style_count):\n            if i < self.coarse_ind:\n                style = GradualStyleBlock(512, 512, 16)\n            elif i < self.middle_ind:\n                style = GradualStyleBlock(512, 512, 32)\n            else:\n                style = GradualStyleBlock(512, 512, 64)\n            self.styles.append(style)\n        self.latlayer1 = nn.Conv2d(256, 512, kernel_size=1, stride=1, padding=0)\n        self.latlayer2 = nn.Conv2d(128, 512, kernel_size=1, stride=1, padding=0)\n        self.alpha = 0.2\n\n    def _upsample_add(self, x, y):\n        '''Upsample and add two feature maps.\n        Args:\n          x: (Variable) top feature map to be upsampled.\n          y: (Variable) lateral feature map.\n        Returns:\n          (Variable) added feature map.\n        Note in PyTorch, when input size is odd, the upsampled feature map\n        with `F.upsample(..., scale_factor=2, mode='nearest')`\n        maybe not equal to the lateral feature map size.\n        e.g.\n        original input size: [N,_,15,15] ->\n        conv2d feature map size: [N,_,8,8] ->\n        upsampled feature map size: [N,_,16,16]\n        So we choose bilinear upsample which supports arbitrary output sizes.\n        '''\n        _, _, H, W = y.size()\n        return F.interpolate(x, size=(H, W), mode='bilinear', align_corners=True) + y\n\n    def forward(self, x, y_hist):\n        #condition = self.condition_layer(y_hist)\n        #print(condition.shape)\n        #print(f\"{x.shape}: xshape\")\n        #print(f\"{y_hist.shape}: y_hist shape\")\n        #print(f\"{condition.shape}: condition shape\")\n        #print(condition.shape)\n        # batch size = 4\n        x = self.input_layer(x)\n\n        latents = []\n        modulelist = list(self.body._modules.values())\n        for i, l in enumerate(modulelist):\n            x = l(x)\n            if i == 6:\n                c1 = x\n            elif i == 20:\n                c2 = x\n            elif i == 23:\n                c3 = x\n        \n        #c1_conditioned = torch.cat((c1, condition), dim=1)\n        \n        #c1_conditioned = self.conv(c1_conditioned)\n\n        for j in range(self.coarse_ind):\n            latents.append(self.styles[j](c3))\n        \n        out = torch.stack(latents, dim=1)\n\n        p2 = self._upsample_add(c3, self.latlayer1(c2))\n        for j in range(self.coarse_ind, self.middle_ind):\n            latents.append(self.styles[j](p2))\n        \n        out = torch.stack(latents, dim=1)\n\n        p1 = self._upsample_add(p2, self.latlayer2(c1))\n        \n        p1_conditioned = torch.cat((p1, y_hist), dim=1)\n        \n        p1_conditioned = self.conv(p1_conditioned)\n        \n        \n        for j in range(self.middle_ind, self.style_count):\n            latents.append(self.styles[j](p1_conditioned))\n            \n\n        out = torch.stack(latents, dim=1)\n        return out\n\n\"\"\"\nAfter input layer:  torch.Size([4, 64, 256, 256])\nc1 shape:  torch.Size([4, 128, 64, 64])\nc2 shape:  torch.Size([4, 256, 32, 32])\nc3 shape:  torch.Size([4, 512, 16, 16])\nAfter coarse layers:  torch.Size([4, 3, 512])\np2 shape:  torch.Size([4, 512, 32, 32])\nAfter mid layers:  torch.Size([4, 7, 512])\np1 shape:  torch.Size([4, 512, 64, 64])\nOutput shape:  torch.Size([4, 18, 512])\n\"\"\"\nclass BackboneEncoderUsingLastLayerIntoW(Module):\n    def __init__(self, num_layers, mode='ir', opts=None):\n        super(BackboneEncoderUsingLastLayerIntoW, self).__init__()\n        print('Using BackboneEncoderUsingLastLayerIntoW')\n        assert num_layers in [50, 100, 152], 'num_layers should be 50,100, or 152'\n        assert mode in ['ir', 'ir_se'], 'mode should be ir or ir_se'\n        blocks = get_blocks(num_layers)\n        if mode == 'ir':\n            unit_module = bottleneck_IR\n        elif mode == 'ir_se':\n            unit_module = bottleneck_IR_SE\n        self.input_layer = Sequential(Conv2d(opts.input_nc, 64, (3, 3), 1, 1, bias=False),\n                                      BatchNorm2d(64),\n                                      PReLU(64))\n        self.output_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n        self.linear = EqualLinear(512, 512, lr_mul=1)\n        modules = []\n        for block in blocks:\n            for bottleneck in block:\n                modules.append(unit_module(bottleneck.in_channel,\n                                           bottleneck.depth,\n                                           bottleneck.stride))\n        self.body = Sequential(*modules)\n\n    def forward(self, x):\n        x = self.input_layer(x)\n        x = self.body(x)\n        x = self.output_pool(x)\n        x = x.view(-1, 512)\n        x = self.linear(x)\n        return x\n\n\nclass BackboneEncoderUsingLastLayerIntoWPlus(Module):\n    def __init__(self, num_layers, mode='ir', opts=None):\n        super(BackboneEncoderUsingLastLayerIntoWPlus, self).__init__()\n        print('Using BackboneEncoderUsingLastLayerIntoWPlus')\n        assert num_layers in [50, 100, 152], 'num_layers should be 50,100, or 152'\n        assert mode in ['ir', 'ir_se'], 'mode should be ir or ir_se'\n        blocks = get_blocks(num_layers)\n        if mode == 'ir':\n            unit_module = bottleneck_IR\n        elif mode == 'ir_se':\n            unit_module = bottleneck_IR_SE\n        self.n_styles = opts.n_styles\n        self.input_layer = Sequential(Conv2d(opts.input_nc, 64, (3, 3), 1, 1, bias=False),\n                                      BatchNorm2d(64),\n                                      PReLU(64))\n        self.output_layer_2 = Sequential(BatchNorm2d(512),\n                                         torch.nn.AdaptiveAvgPool2d((7, 7)),\n                                         Flatten(),\n                                         Linear(512 * 7 * 7, 512))\n        self.linear = EqualLinear(512, 512 * self.n_styles, lr_mul=1)\n        modules = []\n        for block in blocks:\n            for bottleneck in block:\n                modules.append(unit_module(bottleneck.in_channel,\n                                           bottleneck.depth,\n                                           bottleneck.stride))\n        self.body = Sequential(*modules)\n\n    def forward(self, x):\n        x = self.input_layer(x)\n        x = self.body(x)\n        x = self.output_layer_2(x)\n        x = self.linear(x)\n        x = x.view(-1, self.n_styles, 512)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:46:16.897478Z","iopub.execute_input":"2024-05-19T08:46:16.897823Z","iopub.status.idle":"2024-05-19T08:46:16.909781Z","shell.execute_reply.started":"2024-05-19T08:46:16.897798Z","shell.execute_reply":"2024-05-19T08:46:16.908778Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/pixel2style2pixel/models/encoders/psp_encoders.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -r /kaggle/working/pixel2style2pixel/exp/run1","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:52:11.595888Z","iopub.execute_input":"2024-05-19T08:52:11.596595Z","iopub.status.idle":"2024-05-19T08:52:12.704090Z","shell.execute_reply.started":"2024-05-19T08:52:11.596546Z","shell.execute_reply":"2024-05-19T08:52:12.703031Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"!python scripts/train.py \\\n--dataset_type=ffhq_encode \\\n--exp_dir=exp/run1 \\\n--workers=4 \\\n--batch_size=4 \\\n--test_batch_size=4 \\\n--test_workers=4 \\\n--val_interval=10000 \\\n--save_interval=10000 \\\n--encoder_type=GradualStyleEncoder \\\n--start_from_latent_avg \\\n--lpips_lambda=0.8 \\\n--l2_lambda=1 \\\n--id_lambda=0.1 \\\n--image_interval=100 ","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:52:12.706059Z","iopub.execute_input":"2024-05-19T08:52:12.706357Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"2024-05-19 08:52:16.581206: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-19 08:52:16.581268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-19 08:52:16.582849: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n{'batch_size': 4,\n 'board_interval': 50,\n 'checkpoint_path': None,\n 'dataset_type': 'ffhq_encode',\n 'encoder_type': 'GradualStyleEncoder',\n 'exp_dir': 'exp/run1',\n 'id_lambda': 0.1,\n 'image_interval': 100,\n 'input_nc': 3,\n 'l2_lambda': 1.0,\n 'l2_lambda_crop': 0,\n 'label_nc': 0,\n 'learn_in_w': False,\n 'learning_rate': 0.0001,\n 'lpips_lambda': 0.8,\n 'lpips_lambda_crop': 0,\n 'max_steps': 500000,\n 'moco_lambda': 0,\n 'optim_name': 'ranger',\n 'output_size': 1024,\n 'resize_factors': None,\n 'save_interval': 10000,\n 'start_from_latent_avg': True,\n 'stylegan_weights': 'pretrained_models/stylegan2-ffhq-config-f.pt',\n 'test_batch_size': 4,\n 'test_workers': 4,\n 'train_decoder': False,\n 'use_wandb': False,\n 'val_interval': 10000,\n 'w_norm_lambda': 0,\n 'workers': 4}\nLoading encoders weights from irse50!\nLoading decoder weights from pretrained!\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nLoading ResNet ArcFace\nLoading dataset for ffhq_encode\nNumber of training samples: 24000\nNumber of test samples: 6000\n/kaggle/working/pixel2style2pixel/./training/ranger.py:123: UserWarning: This overload of addcmul_ is deprecated:\n\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\nConsider using one of the following signatures instead:\n\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/python_arg_parser.cpp:1519.)\n  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\nMetrics for train, step 0\n\tloss_id =  0.9456934928894043\n\tid_improve =  -0.9263748968951404\n\tloss_l2 =  0.5374895334243774\n\tloss_lpips =  0.5520044565200806\n\thist_loss =  0.004169872496277094\n\tloss =  1.0778322219848633\nMetrics for train, step 50\n\tloss_id =  1.0046191215515137\n\tid_improve =  -0.982526330742985\n\tloss_l2 =  0.28777015209198\n\tloss_lpips =  0.4578770101070404\n\thist_loss =  0.004738859832286835\n\tloss =  0.7592725157737732\nMetrics for train, step 100\n\tloss_id =  0.9261501431465149\n\tid_improve =  -0.9096265104599297\n\tloss_l2 =  0.2457791268825531\n\tloss_lpips =  0.41566240787506104\n\thist_loss =  0.004572881385684013\n\tloss =  0.6754969358444214\nMetrics for train, step 150\n\tloss_id =  0.9340023398399353\n\tid_improve =  -0.9095193687826395\n\tloss_l2 =  0.21190792322158813\n\tloss_lpips =  0.48537755012512207\n\thist_loss =  0.004452478606253862\n\tloss =  0.6980626583099365\nMetrics for train, step 200\n\tloss_id =  0.9069069623947144\n\tid_improve =  -0.89288857486099\n\tloss_l2 =  0.22463524341583252\n\tloss_lpips =  0.44603729248046875\n\thist_loss =  0.004304513335227966\n\tloss =  0.676460325717926\nMetrics for train, step 250\n\tloss_id =  0.9648638963699341\n\tid_improve =  -0.9485045173205435\n\tloss_l2 =  0.2839965522289276\n\tloss_lpips =  0.447002112865448\n\thist_loss =  0.004537491127848625\n\tloss =  0.7426221370697021\nMetrics for train, step 300\n\tloss_id =  0.899429440498352\n\tid_improve =  -0.8834345364011824\n\tloss_l2 =  0.2207142561674118\n\tloss_lpips =  0.4225613474845886\n\thist_loss =  0.004590732976794243\n\tloss =  0.6532970666885376\nMetrics for train, step 350\n\tloss_id =  0.9282910823822021\n\tid_improve =  -0.9114442374557257\n\tloss_l2 =  0.18382811546325684\n\tloss_lpips =  0.4083763659000397\n\thist_loss =  0.0042351894080638885\n\tloss =  0.6075935363769531\nMetrics for train, step 400\n\tloss_id =  0.9181151390075684\n\tid_improve =  -0.900723846629262\n\tloss_l2 =  0.20107358694076538\n\tloss_lpips =  0.4051585793495178\n\thist_loss =  0.004439381882548332\n\tloss =  0.6214513182640076\nMetrics for train, step 450\n\tloss_id =  0.8981618881225586\n\tid_improve =  -0.880789140239358\n\tloss_l2 =  0.27702876925468445\n\tloss_lpips =  0.44292229413986206\n\thist_loss =  0.004195678047835827\n\tloss =  0.7253785133361816\nMetrics for train, step 500\n\tloss_id =  0.837452232837677\n\tid_improve =  -0.8144180476665497\n\tloss_l2 =  0.20562848448753357\n\tloss_lpips =  0.413426011800766\n\thist_loss =  0.004413994960486889\n\tloss =  0.6245285272598267\nMetrics for train, step 550\n\tloss_id =  0.8857294321060181\n\tid_improve =  -0.8631601752713323\n\tloss_l2 =  0.20199519395828247\n\tloss_lpips =  0.39450129866600037\n\thist_loss =  0.004554839804768562\n\tloss =  0.6107240915298462\nMetrics for train, step 600\n\tloss_id =  0.817103385925293\n\tid_improve =  -0.797085203230381\n\tloss_l2 =  0.26575109362602234\n\tloss_lpips =  0.3639482259750366\n\thist_loss =  0.0048461612313985825\n\tloss =  0.6434661746025085\nMetrics for train, step 650\n\tloss_id =  0.9968221187591553\n\tid_improve =  -0.9723577564582229\n\tloss_l2 =  0.23534545302391052\n\tloss_lpips =  0.43291354179382324\n\thist_loss =  0.004563061520457268\n\tloss =  0.6859215497970581\nMetrics for train, step 700\n\tloss_id =  0.8806802034378052\n\tid_improve =  -0.8673820625990629\n\tloss_l2 =  0.18504826724529266\n\tloss_lpips =  0.37963342666625977\n\thist_loss =  0.004194764420390129\n\tloss =  0.5810177326202393\nMetrics for train, step 750\n\tloss_id =  0.8865814805030823\n\tid_improve =  -0.8678714130073786\n\tloss_l2 =  0.1640501320362091\n\tloss_lpips =  0.3950226902961731\n\thist_loss =  0.004046273417770863\n\tloss =  0.5727726817131042\nMetrics for train, step 800\n\tloss_id =  0.9446481466293335\n\tid_improve =  -0.9286540059838444\n\tloss_l2 =  0.1798127293586731\n\tloss_lpips =  0.4107564389705658\n\thist_loss =  0.004224679432809353\n\tloss =  0.6071073412895203\nMetrics for train, step 850\n\tloss_id =  0.8719444274902344\n\tid_improve =  -0.8466468560509384\n\tloss_l2 =  0.189753457903862\n\tloss_lpips =  0.33079373836517334\n\thist_loss =  0.005124897230416536\n\tloss =  0.5467078685760498\nMetrics for train, step 900\n\tloss_id =  0.9658942222595215\n\tid_improve =  -0.9516244973056018\n\tloss_l2 =  0.15546175837516785\n\tloss_lpips =  0.3706069886684418\n\thist_loss =  0.004103310406208038\n\tloss =  0.5526400804519653\nMetrics for train, step 950\n\tloss_id =  0.9383179545402527\n\tid_improve =  -0.9208073785994202\n\tloss_l2 =  0.1586006134748459\n\tloss_lpips =  0.37717071175575256\n\thist_loss =  0.004349767230451107\n\tloss =  0.5585187077522278\nMetrics for train, step 1000\n\tloss_id =  0.9141393303871155\n\tid_improve =  -0.9042412643320858\n\tloss_l2 =  0.16808077692985535\n\tloss_lpips =  0.36951613426208496\n\thist_loss =  0.004043685272336006\n\tloss =  0.559151291847229\nMetrics for train, step 1050\n\tloss_id =  0.8976808190345764\n\tid_improve =  -0.8801383329555392\n\tloss_l2 =  0.14262785017490387\n\tloss_lpips =  0.30259469151496887\n\thist_loss =  0.004168256651610136\n\tloss =  0.4786399304866791\nMetrics for train, step 1100\n\tloss_id =  0.8769199848175049\n\tid_improve =  -0.8615891933441162\n\tloss_l2 =  0.09213566780090332\n\tloss_lpips =  0.3310897946357727\n\thist_loss =  0.003932168707251549\n\tloss =  0.4486316740512848\nMetrics for train, step 1150\n\tloss_id =  0.9229437112808228\n\tid_improve =  -0.9027617033571005\n\tloss_l2 =  0.11841161549091339\n\tloss_lpips =  0.3504132926464081\n\thist_loss =  0.004082530736923218\n\tloss =  0.4951191544532776\nMetrics for train, step 1200\n\tloss_id =  0.9263993501663208\n\tid_improve =  -0.9105563806369901\n\tloss_l2 =  0.17034181952476501\n\tloss_lpips =  0.37173134088516235\n\thist_loss =  0.0040144892409443855\n\tloss =  0.5643813610076904\nMetrics for train, step 1250\n\tloss_id =  0.8787041902542114\n\tid_improve =  -0.8583408575505018\n\tloss_l2 =  0.11968813836574554\n\tloss_lpips =  0.337257981300354\n\thist_loss =  0.0042550526559352875\n\tloss =  0.48162001371383667\nMetrics for train, step 1300\n\tloss_id =  0.8744245767593384\n\tid_improve =  -0.8569080708548427\n\tloss_l2 =  0.13433822989463806\n\tloss_lpips =  0.3618127703666687\n\thist_loss =  0.0046712374314665794\n\tloss =  0.5159021615982056\nMetrics for train, step 1350\n\tloss_id =  0.9388529062271118\n\tid_improve =  -0.9260730925016105\n\tloss_l2 =  0.09947224706411362\n\tloss_lpips =  0.3632251024246216\n\thist_loss =  0.004270408768206835\n\tloss =  0.48820802569389343\nMetrics for train, step 1400\n\tloss_id =  0.9164546132087708\n\tid_improve =  -0.8972165007144213\n\tloss_l2 =  0.17739777266979218\n\tloss_lpips =  0.3918347656726837\n\thist_loss =  0.0041189538314938545\n\tloss =  0.5866300463676453\nMetrics for train, step 1450\n\tloss_id =  0.8657253384590149\n\tid_improve =  -0.8508423129096627\n\tloss_l2 =  0.09652857482433319\n\tloss_lpips =  0.28062954545021057\n\thist_loss =  0.004029865376651287\n\tloss =  0.411634624004364\nMetrics for train, step 1500\n\tloss_id =  0.8231964111328125\n\tid_improve =  -0.8080294486135244\n\tloss_l2 =  0.11345010250806808\n\tloss_lpips =  0.3342355191707611\n\thist_loss =  0.004083085805177689\n\tloss =  0.4672412872314453\nMetrics for train, step 1550\n\tloss_id =  0.8748266100883484\n\tid_improve =  -0.8556391624733806\n\tloss_l2 =  0.1259465217590332\n\tloss_lpips =  0.34627991914749146\n\thist_loss =  0.004251900129020214\n\tloss =  0.4947050213813782\nMetrics for train, step 1600\n\tloss_id =  0.9632991552352905\n\tid_improve =  -0.9458896666765213\n\tloss_l2 =  0.12051457166671753\n\tloss_lpips =  0.319095253944397\n\thist_loss =  0.004066119436174631\n\tloss =  0.4761868119239807\nMetrics for train, step 1650\n\tloss_id =  1.0553172826766968\n\tid_improve =  -1.0383315144572407\n\tloss_l2 =  0.15610921382904053\n\tloss_lpips =  0.322653591632843\n\thist_loss =  0.004186591133475304\n\tloss =  0.5239503979682922\nMetrics for train, step 1700\n\tloss_id =  0.8980663418769836\n\tid_improve =  -0.8850929940235801\n\tloss_l2 =  0.1085774153470993\n\tloss_lpips =  0.3087507486343384\n\thist_loss =  0.004059484228491783\n\tloss =  0.44944414496421814\nMetrics for train, step 1750\n\tloss_id =  0.9131569862365723\n\tid_improve =  -0.8978746719658375\n\tloss_l2 =  0.12293162196874619\n\tloss_lpips =  0.3473731279373169\n\thist_loss =  0.004108704160898924\n\tloss =  0.4962545335292816\nMetrics for train, step 1800\n\tloss_id =  0.9338994026184082\n\tid_improve =  -0.9119559028185904\n\tloss_l2 =  0.09086287021636963\n\tloss_lpips =  0.3114835023880005\n\thist_loss =  0.004654429852962494\n\tloss =  0.43809404969215393\nMetrics for train, step 1850\n\tloss_id =  0.9253916144371033\n\tid_improve =  -0.9102938715368509\n\tloss_l2 =  0.09837613999843597\n\tloss_lpips =  0.26193660497665405\n\thist_loss =  0.004155762959271669\n\tloss =  0.40462034940719604\nMetrics for train, step 1900\n\tloss_id =  0.8944787979125977\n\tid_improve =  -0.8701729495078325\n\tloss_l2 =  0.12845994532108307\n\tloss_lpips =  0.3449499011039734\n\thist_loss =  0.00410050991922617\n\tloss =  0.49796825647354126\nMetrics for train, step 1950\n\tloss_id =  0.8845557570457458\n\tid_improve =  -0.8666405030526221\n\tloss_l2 =  0.12580114603042603\n\tloss_lpips =  0.32736366987228394\n\thist_loss =  0.004255083855241537\n\tloss =  0.48040276765823364\nMetrics for train, step 2000\n\tloss_id =  0.9809991121292114\n\tid_improve =  -0.9624746195040643\n\tloss_l2 =  0.19943439960479736\n\tloss_lpips =  0.4167836308479309\n\thist_loss =  0.004373075440526009\n\tloss =  0.635334312915802\nMetrics for train, step 2050\n\tloss_id =  0.9354265928268433\n\tid_improve =  -0.9241783670149744\n\tloss_l2 =  0.11305680125951767\n\tloss_lpips =  0.35552388429641724\n\thist_loss =  0.003976635634899139\n\tloss =  0.49499523639678955\nMetrics for train, step 2100\n\tloss_id =  0.8592981100082397\n\tid_improve =  -0.8401364544406533\n\tloss_l2 =  0.10393400490283966\n\tloss_lpips =  0.28476062417030334\n\thist_loss =  0.004117934033274651\n\tloss =  0.4217902719974518\nMetrics for train, step 2150\n\tloss_id =  0.8060594201087952\n\tid_improve =  -0.7902799509465694\n\tloss_l2 =  0.1409274786710739\n\tloss_lpips =  0.3497401177883148\n\thist_loss =  0.004071049392223358\n\tloss =  0.5053965449333191\nMetrics for train, step 2200\n\tloss_id =  0.7785886526107788\n\tid_improve =  -0.7600768022239208\n\tloss_l2 =  0.11517606675624847\n\tloss_lpips =  0.3294609785079956\n\thist_loss =  0.004509084392338991\n\tloss =  0.4611127972602844\nMetrics for train, step 2250\n\tloss_id =  0.8510029315948486\n\tid_improve =  -0.8388397060334682\n\tloss_l2 =  0.09813528507947922\n\tloss_lpips =  0.2757016122341156\n\thist_loss =  0.004151566885411739\n\tloss =  0.40794843435287476\nMetrics for train, step 2300\n\tloss_id =  0.8412704467773438\n\tid_improve =  -0.8114765640348196\n\tloss_l2 =  0.17200617492198944\n\tloss_lpips =  0.37740451097488403\n\thist_loss =  0.004477931186556816\n\tloss =  0.562534749507904\nMetrics for train, step 2350\n\tloss_id =  0.8007895946502686\n\tid_improve =  -0.7807172406464815\n\tloss_l2 =  0.135343998670578\n\tloss_lpips =  0.3476306200027466\n\thist_loss =  0.004081966821104288\n\tloss =  0.4976094365119934\nMetrics for train, step 2400\n\tloss_id =  0.9524228572845459\n\tid_improve =  -0.9352287109941244\n\tloss_l2 =  0.08688338100910187\n\tloss_lpips =  0.3046301603317261\n\thist_loss =  0.00399808818474412\n\tloss =  0.42982786893844604\nMetrics for train, step 2450\n\tloss_id =  0.871509313583374\n\tid_improve =  -0.8495109500363469\n\tloss_l2 =  0.1091192215681076\n\tloss_lpips =  0.3784942924976349\n\thist_loss =  0.004187369719147682\n\tloss =  0.5032529234886169\nMetrics for train, step 2500\n\tloss_id =  0.9279181957244873\n\tid_improve =  -0.9018239211291075\n\tloss_l2 =  0.16804933547973633\n\tloss_lpips =  0.34706953167915344\n\thist_loss =  0.00442195450887084\n\tloss =  0.5429187417030334\nMetrics for train, step 2550\n\tloss_id =  0.8777506351470947\n\tid_improve =  -0.8637594655156136\n\tloss_l2 =  0.12048636376857758\n\tloss_lpips =  0.2853696346282959\n\thist_loss =  0.004870730452239513\n\tloss =  0.44142788648605347\nMetrics for train, step 2600\n\tloss_id =  0.9622038006782532\n\tid_improve =  -0.9467754848301411\n\tloss_l2 =  0.12323169410228729\n\tloss_lpips =  0.3250422775745392\n\thist_loss =  0.004038777202367783\n\tloss =  0.4835246801376343\nMetrics for train, step 2650\n\tloss_id =  0.8458842635154724\n\tid_improve =  -0.828162768855691\n\tloss_l2 =  0.10763572156429291\n\tloss_lpips =  0.3292228877544403\n\thist_loss =  0.004509201738983393\n\tloss =  0.46011167764663696\nMetrics for train, step 2700\n\tloss_id =  0.9096664190292358\n\tid_improve =  -0.894889747723937\n\tloss_l2 =  0.08318834006786346\n\tloss_lpips =  0.3378259539604187\n\thist_loss =  0.004004010930657387\n\tloss =  0.4484197497367859\nMetrics for train, step 2750\n\tloss_id =  0.7971401214599609\n\tid_improve =  -0.7765288352966309\n\tloss_l2 =  0.13384182751178741\n\tloss_lpips =  0.3877761960029602\n\thist_loss =  0.004050270654261112\n\tloss =  0.5278270244598389\nMetrics for train, step 2800\n\tloss_id =  0.8325678110122681\n\tid_improve =  -0.8175726123154163\n\tloss_l2 =  0.10213114321231842\n\tloss_lpips =  0.3080129027366638\n\thist_loss =  0.003973308485001326\n\tloss =  0.43577155470848083\nMetrics for train, step 2850\n\tloss_id =  0.8313295841217041\n\tid_improve =  -0.8132458850741386\n\tloss_l2 =  0.0859442800283432\n\tloss_lpips =  0.3347596526145935\n\thist_loss =  0.00394866056740284\n\tloss =  0.4408336281776428\nMetrics for train, step 2900\n\tloss_id =  0.8530126214027405\n\tid_improve =  -0.8360844068229198\n\tloss_l2 =  0.11830690503120422\n\tloss_lpips =  0.3453846275806427\n\thist_loss =  0.004120197147130966\n\tloss =  0.48403605818748474\nMetrics for train, step 2950\n\tloss_id =  0.9069970846176147\n\tid_improve =  -0.8917090496979654\n\tloss_l2 =  0.09414898604154587\n\tloss_lpips =  0.321573406457901\n\thist_loss =  0.003968966193497181\n\tloss =  0.4460763931274414\nMetrics for train, step 3000\n\tloss_id =  0.9068264961242676\n\tid_improve =  -0.8869618345052004\n\tloss_l2 =  0.08892747014760971\n\tloss_lpips =  0.328174889087677\n\thist_loss =  0.003926271107047796\n\tloss =  0.44607633352279663\nMetrics for train, step 3050\n\tloss_id =  0.8378061652183533\n\tid_improve =  -0.817530739121139\n\tloss_l2 =  0.0906287357211113\n\tloss_lpips =  0.28581535816192627\n\thist_loss =  0.004317792132496834\n\tloss =  0.4073794186115265\nMetrics for train, step 3100\n\tloss_id =  0.8289568424224854\n\tid_improve =  -0.8099313452839851\n\tloss_l2 =  0.08785807341337204\n\tloss_lpips =  0.32209885120391846\n\thist_loss =  0.004028826020658016\n\tloss =  0.4324616491794586\nMetrics for train, step 3150\n\tloss_id =  0.8057090044021606\n\tid_improve =  -0.7922357004135847\n\tloss_l2 =  0.10359414666891098\n\tloss_lpips =  0.31696560978889465\n\thist_loss =  0.0044499789364635944\n\tloss =  0.44218751788139343\nMetrics for train, step 3200\n\tloss_id =  0.8750786185264587\n\tid_improve =  -0.8591193514876068\n\tloss_l2 =  0.06947600841522217\n\tloss_lpips =  0.2945244312286377\n\thist_loss =  0.00396052235737443\n\tloss =  0.39656394720077515\nMetrics for train, step 3250\n\tloss_id =  0.8705254793167114\n\tid_improve =  -0.850325339473784\n\tloss_l2 =  0.10116701573133469\n\tloss_lpips =  0.33222442865371704\n\thist_loss =  0.004156782757490873\n\tloss =  0.45815587043762207\nMetrics for train, step 3350\n\tloss_id =  0.885951817035675\n\tid_improve =  -0.8667968094814569\n\tloss_l2 =  0.1120220273733139\n\tloss_lpips =  0.29123640060424805\n\thist_loss =  0.004003020003437996\n\tloss =  0.43760934472084045\nMetrics for train, step 3400\n\tloss_id =  0.8100500702857971\n\tid_improve =  -0.7925147075438872\n\tloss_l2 =  0.08735650777816772\n\tloss_lpips =  0.28313010931015015\n\thist_loss =  0.003942510113120079\n\tloss =  0.3988081216812134\nMetrics for train, step 3450\n\tloss_id =  0.8315424919128418\n\tid_improve =  -0.7864595409482718\n\tloss_l2 =  0.13357840478420258\n\tloss_lpips =  0.36026859283447266\n\thist_loss =  0.00424813199788332\n\tloss =  0.5091956853866577\nMetrics for train, step 3500\n\tloss_id =  0.8949300646781921\n\tid_improve =  -0.8744381912983954\n\tloss_l2 =  0.08844475448131561\n\tloss_lpips =  0.3103472888469696\n\thist_loss =  0.00397745706140995\n\tloss =  0.4301930367946625\nMetrics for train, step 3550\n\tloss_id =  0.818657398223877\n\tid_improve =  -0.8032095804810524\n\tloss_l2 =  0.11314475536346436\n\tloss_lpips =  0.3043104112148285\n\thist_loss =  0.004228145349770784\n\tloss =  0.4426869750022888\nMetrics for train, step 3600\n\tloss_id =  0.814203679561615\n\tid_improve =  -0.7957324590533972\n\tloss_l2 =  0.13468065857887268\n\tloss_lpips =  0.3825152516365051\n\thist_loss =  0.0042145815677940845\n\tloss =  0.5263277888298035\nMetrics for train, step 3650\n\tloss_id =  0.8478552103042603\n\tid_improve =  -0.8294071443378925\n\tloss_l2 =  0.10042965412139893\n\tloss_lpips =  0.33001431822776794\n\thist_loss =  0.004111385438591242\n\tloss =  0.45333802700042725\nMetrics for train, step 3700\n\tloss_id =  0.9584100246429443\n\tid_improve =  -0.9418072681874037\n\tloss_l2 =  0.08901342004537582\n\tloss_lpips =  0.3480377495288849\n\thist_loss =  0.004245268180966377\n\tloss =  0.46752989292144775\nMetrics for train, step 3750\n\tloss_id =  0.9017648100852966\n\tid_improve =  -0.8834710185183212\n\tloss_l2 =  0.07439122349023819\n\tloss_lpips =  0.24456490576267242\n\thist_loss =  0.004324403591454029\n\tloss =  0.3645440638065338\nMetrics for train, step 3800\n\tloss_id =  0.8788408041000366\n\tid_improve =  -0.8623807250987738\n\tloss_l2 =  0.08324005454778671\n\tloss_lpips =  0.2923370599746704\n\thist_loss =  0.004087274428457022\n\tloss =  0.40908104181289673\nMetrics for train, step 3850\n\tloss_id =  0.7472502589225769\n\tid_improve =  -0.7187919709831476\n\tloss_l2 =  0.09024753421545029\n\tloss_lpips =  0.32350990176200867\n\thist_loss =  0.004272362217307091\n\tloss =  0.4280528724193573\nMetrics for train, step 3900\n\tloss_id =  0.7630034685134888\n\tid_improve =  -0.7516404737252742\n\tloss_l2 =  0.060637395828962326\n\tloss_lpips =  0.26382505893707275\n\thist_loss =  0.004101331811398268\n\tloss =  0.35209912061691284\nMetrics for train, step 3950\n\tloss_id =  0.8516750335693359\n\tid_improve =  -0.8335234022233635\n\tloss_l2 =  0.08547858893871307\n\tloss_lpips =  0.32081109285354614\n\thist_loss =  0.003948305267840624\n\tloss =  0.43124327063560486\nMetrics for train, step 4000\n\tloss_id =  0.8224195241928101\n\tid_improve =  -0.8139813914895058\n\tloss_l2 =  0.09397095441818237\n\tloss_lpips =  0.3594180941581726\n\thist_loss =  0.004014298319816589\n\tloss =  0.46776169538497925\nMetrics for train, step 4050\n\tloss_id =  0.8773102760314941\n\tid_improve =  -0.8598333615809679\n\tloss_l2 =  0.11195802688598633\n\tloss_lpips =  0.3357030153274536\n\thist_loss =  0.003935785498470068\n\tloss =  0.47218725085258484\nMetrics for train, step 4100\n\tloss_id =  0.8615306615829468\n\tid_improve =  -0.8412557318806648\n\tloss_l2 =  0.09617086499929428\n\tloss_lpips =  0.3264244794845581\n\thist_loss =  0.004037345759570599\n\tloss =  0.44750088453292847\nMetrics for train, step 4150\n\tloss_id =  0.8423098921775818\n\tid_improve =  -0.8230869774706662\n\tloss_l2 =  0.08763577044010162\n\tloss_lpips =  0.27827274799346924\n\thist_loss =  0.004108687397092581\n\tloss =  0.3985936641693115\nMetrics for train, step 4200\n\tloss_id =  0.7269369959831238\n\tid_improve =  -0.7097687870264053\n\tloss_l2 =  0.058687906712293625\n\tloss_lpips =  0.28555798530578613\n\thist_loss =  0.003986794035881758\n\tloss =  0.3638148009777069\nMetrics for train, step 4250\n\tloss_id =  0.7989221811294556\n\tid_improve =  -0.7809216594323516\n\tloss_l2 =  0.10773366689682007\n\tloss_lpips =  0.3319689631462097\n\thist_loss =  0.004063750617206097\n\tloss =  0.45726481080055237\nMetrics for train, step 4300\n\tloss_id =  0.7354980111122131\n\tid_improve =  -0.7042237594723701\n\tloss_l2 =  0.12954501807689667\n\tloss_lpips =  0.3186648190021515\n\thist_loss =  0.004377908539026976\n\tloss =  0.4624045789241791\nMetrics for train, step 4350\n\tloss_id =  0.8338401317596436\n\tid_improve =  -0.8187713455408812\n\tloss_l2 =  0.11541863530874252\n\tloss_lpips =  0.3042120039463043\n\thist_loss =  0.0048398664221167564\n\tloss =  0.44701212644577026\nMetrics for train, step 4400\n\tloss_id =  0.7700356245040894\n\tid_improve =  -0.7565328031778336\n\tloss_l2 =  0.11052894592285156\n\tloss_lpips =  0.30600935220718384\n\thist_loss =  0.0040756030939519405\n\tloss =  0.4364156126976013\nMetrics for train, step 4450\n\tloss_id =  0.8555806875228882\n\tid_improve =  -0.8353461530059576\n\tloss_l2 =  0.0764516219496727\n\tloss_lpips =  0.2820155620574951\n\thist_loss =  0.004343891516327858\n\tloss =  0.3919660151004791\nMetrics for train, step 4500\n\tloss_id =  0.8235104084014893\n\tid_improve =  -0.8100303784012794\n\tloss_l2 =  0.06939949095249176\n\tloss_lpips =  0.2719739079475403\n\thist_loss =  0.003984343260526657\n\tloss =  0.3733139932155609\nMetrics for train, step 4550\n\tloss_id =  0.7526020407676697\n\tid_improve =  -0.726811271160841\n\tloss_l2 =  0.11527854204177856\n\tloss_lpips =  0.3534165620803833\n\thist_loss =  0.0039909640327095985\n\tloss =  0.4772630035877228\nMetrics for train, step 4600\n\tloss_id =  0.7552077770233154\n\tid_improve =  -0.7354613468050957\n\tloss_l2 =  0.06461723148822784\n\tloss_lpips =  0.2828105092048645\n\thist_loss =  0.004240610636770725\n\tloss =  0.3706270158290863\nMetrics for train, step 4650\n\tloss_id =  0.7585833072662354\n\tid_improve =  -0.7386337243951857\n\tloss_l2 =  0.08683805912733078\n\tloss_lpips =  0.29470503330230713\n\thist_loss =  0.00406329520046711\n\tloss =  0.4025237262248993\nMetrics for train, step 4700\n\tloss_id =  0.8126447200775146\n\tid_improve =  -0.7961587831377983\n\tloss_l2 =  0.08472049236297607\n\tloss_lpips =  0.25798627734184265\n\thist_loss =  0.004486456047743559\n\tloss =  0.37686043977737427\nMetrics for train, step 4750\n\tloss_id =  0.869122326374054\n\tid_improve =  -0.8476947005838156\n\tloss_l2 =  0.10482746362686157\n\tloss_lpips =  0.31679582595825195\n\thist_loss =  0.004370176233351231\n\tloss =  0.44954654574394226\nMetrics for train, step 4800\n\tloss_id =  0.8829487562179565\n\tid_improve =  -0.8702376820147038\n\tloss_l2 =  0.09499680250883102\n\tloss_lpips =  0.31151217222213745\n\thist_loss =  0.003999676555395126\n\tloss =  0.4365011155605316\nMetrics for train, step 4850\n\tloss_id =  0.8142631649971008\n\tid_improve =  -0.797873118892312\n\tloss_l2 =  0.07643520832061768\n\tloss_lpips =  0.2521900534629822\n\thist_loss =  0.003972521983087063\n\tloss =  0.3635861277580261\nMetrics for train, step 4900\n\tloss_id =  0.779594898223877\n\tid_improve =  -0.7579075247049332\n\tloss_l2 =  0.09899260848760605\n\tloss_lpips =  0.3032536208629608\n\thist_loss =  0.004168288316577673\n\tloss =  0.4237233102321625\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}