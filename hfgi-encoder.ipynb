{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2177371,"sourceType":"datasetVersion","datasetId":1307206},{"sourceId":31184,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":26158},{"sourceId":35468,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":29847}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Tengfei-Wang/HFGI","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:02:49.698414Z","iopub.execute_input":"2024-04-22T22:02:49.698779Z","iopub.status.idle":"2024-04-22T22:02:52.248400Z","shell.execute_reply.started":"2024-04-22T22:02:49.698748Z","shell.execute_reply":"2024-04-22T22:02:52.247120Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'HFGI'...\nremote: Enumerating objects: 234, done.\u001b[K\nremote: Counting objects: 100% (142/142), done.\u001b[K\nremote: Compressing objects: 100% (104/104), done.\u001b[K\nremote: Total 234 (delta 41), reused 126 (delta 31), pack-reused 92\u001b[K\nReceiving objects: 100% (234/234), 15.81 MiB | 23.88 MiB/s, done.\nResolving deltas: 100% (45/45), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/HFGI","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:02:52.250717Z","iopub.execute_input":"2024-04-22T22:02:52.251222Z","iopub.status.idle":"2024-04-22T22:02:52.258756Z","shell.execute_reply.started":"2024-04-22T22:02:52.251180Z","shell.execute_reply":"2024-04-22T22:02:52.257646Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/HFGI\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir pretrained","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:02:52.264586Z","iopub.execute_input":"2024-04-22T22:02:52.264914Z","iopub.status.idle":"2024-04-22T22:02:53.236210Z","shell.execute_reply.started":"2024-04-22T22:02:52.264884Z","shell.execute_reply":"2024-04-22T22:02:53.234331Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:02:56.843397Z","iopub.execute_input":"2024-04-22T22:02:56.844157Z","iopub.status.idle":"2024-04-22T22:03:10.470046Z","shell.execute_reply.started":"2024-04-22T22:02:56.844119Z","shell.execute_reply":"2024-04-22T22:03:10.469034Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.1.0-py3-none-any.whl (17 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install ninja","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:03:10.471924Z","iopub.execute_input":"2024-04-22T22:03:10.472240Z","iopub.status.idle":"2024-04-22T22:03:22.903743Z","shell.execute_reply.started":"2024-04-22T22:03:10.472212Z","shell.execute_reply":"2024-04-22T22:03:22.902461Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (1.11.1.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip uninstall -y datasets","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:03:22.905620Z","iopub.execute_input":"2024-04-22T22:03:22.905993Z","iopub.status.idle":"2024-04-22T22:03:24.970139Z","shell.execute_reply.started":"2024-04-22T22:03:22.905956Z","shell.execute_reply":"2024-04-22T22:03:24.968969Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Found existing installation: datasets 2.18.0\nUninstalling datasets-2.18.0:\n  Successfully uninstalled datasets-2.18.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT \n!mv /kaggle/working/HFGI/stylegan2-ffhq-config-f.pt /kaggle/working/HFGI/pretrained","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:03:24.972944Z","iopub.execute_input":"2024-04-22T22:03:24.973359Z","iopub.status.idle":"2024-04-22T22:03:31.481782Z","shell.execute_reply.started":"2024-04-22T22:03:24.973320Z","shell.execute_reply":"2024-04-22T22:03:31.480446Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT\nFrom (redirected): https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT&confirm=t&uuid=6c016eaf-502f-4fc9-b8c6-c39d7cc98f0d\nTo: /kaggle/working/HFGI/stylegan2-ffhq-config-f.pt\n100%|█████████████████████████████████████████| 381M/381M [00:03<00:00, 122MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1KW7bjndL3QG3sxBbZxreGHigcCCpsDgn\n!mv /kaggle/working/HFGI/model_ir_se50.pth /kaggle/working/HFGI/pretrained","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:03:31.483444Z","iopub.execute_input":"2024-04-22T22:03:31.483778Z","iopub.status.idle":"2024-04-22T22:03:37.536192Z","shell.execute_reply.started":"2024-04-22T22:03:31.483749Z","shell.execute_reply":"2024-04-22T22:03:37.534887Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1KW7bjndL3QG3sxBbZxreGHigcCCpsDgn\nFrom (redirected): https://drive.google.com/uc?id=1KW7bjndL3QG3sxBbZxreGHigcCCpsDgn&confirm=t&uuid=41dcfd17-c95d-4d63-b030-645ce2d5d0fa\nTo: /kaggle/working/HFGI/model_ir_se50.pth\n100%|████████████████████████████████████████| 175M/175M [00:03<00:00, 57.0MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile configs/transforms_config.py\nfrom abc import abstractmethod\nimport torchvision.transforms as transforms\n\n\nclass TransformsConfig(object):\n\n\tdef __init__(self, opts):\n\t\tself.opts = opts\n\n\t@abstractmethod\n\tdef get_transforms(self):\n\t\tpass\n\n\nclass EncodeTransforms(TransformsConfig):\n\n\tdef __init__(self, opts):\n\t\tsuper(EncodeTransforms, self).__init__(opts)\n\n\tdef get_transforms(self):\n\t\ttransforms_dict = {\n\t\t\t'transform_gt_train': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_source': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n                transforms.Grayscale(num_output_channels=3),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_test': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_inference': transforms.Compose([\n\t\t\t\ttransforms.Resize((256, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n\t\t}\n\t\treturn transforms_dict\n\n\nclass CarsEncodeTransforms(TransformsConfig):\n\n\tdef __init__(self, opts):\n\t\tsuper(CarsEncodeTransforms, self).__init__(opts)\n\n\tdef get_transforms(self):\n\t\ttransforms_dict = {\n\t\t\t'transform_gt_train': transforms.Compose([\n\t\t\t\ttransforms.Resize((192, 256)),\n\t\t\t\ttransforms.RandomHorizontalFlip(0.5),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_source': None,\n\t\t\t'transform_test': transforms.Compose([\n\t\t\t\ttransforms.Resize((192, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n\t\t\t'transform_inference': transforms.Compose([\n\t\t\t\ttransforms.Resize((192, 256)),\n\t\t\t\ttransforms.ToTensor(),\n\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n\t\t}\n\t\treturn transforms_dict\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:03:38.615572Z","iopub.execute_input":"2024-04-22T22:03:38.616279Z","iopub.status.idle":"2024-04-22T22:03:38.622771Z","shell.execute_reply.started":"2024-04-22T22:03:38.616247Z","shell.execute_reply":"2024-04-22T22:03:38.621860Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Overwriting configs/transforms_config.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile configs/paths_config.py\ndataset_paths = {\n\t#  Face Datasets (FFHQ - train, CelebA-HQ - test)\n\t'ffhq': '/kaggle/input/celebahq-resized-256x256',\n\t'ffhq_val': '/kaggle/working/HFGI/test_imgs',\n\n\t#  Cars Dataset (Stanford cars)\n\t'cars_train': '',\n\t'cars_val': '',\n}\n\nmodel_paths = {\n\t'stylegan_ffhq': './pretrained/stylegan2-ffhq-config-f.pt',\n\t'ir_se50': './pretrained/model_ir_se50.pth',\n\t'shape_predictor': './pretrained/shape_predictor_68_face_landmarks.dat',\n\t'moco': './pretrained/moco_v2_800ep_pretrain.pt'\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:03:41.200925Z","iopub.execute_input":"2024-04-22T22:03:41.201298Z","iopub.status.idle":"2024-04-22T22:03:41.208263Z","shell.execute_reply.started":"2024-04-22T22:03:41.201271Z","shell.execute_reply":"2024-04-22T22:03:41.207104Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Overwriting configs/paths_config.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/HFGI/training/coach.py\nimport os\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nmatplotlib.use('Agg')\nimport numpy as np\nimport torch\nfrom torch import nn, autograd\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.functional as F\n\nfrom utils import common, train_utils\nfrom criteria import id_loss, moco_loss\nfrom configs import data_configs\nfrom datasets.images_dataset import ImagesDataset\nfrom criteria.lpips.lpips import LPIPS\nfrom models.psp import pSp\nfrom training.ranger import Ranger\n\nrandom.seed(0)\ntorch.manual_seed(0)\n\n\nclass Coach:\n    def __init__(self, opts):\n        self.opts = opts\n        self.global_step = 0\n        self.device = 'cuda'\n        self.opts.device = self.device\n        self.net = pSp(self.opts).to(self.device) # modify it to your basic encoder\n\n        # Initialize loss\n        if self.opts.lpips_lambda > 0:\n            self.lpips_loss = LPIPS(net_type=self.opts.lpips_type).to(self.device).eval()\n        if self.opts.id_lambda > 0:\n            if 'ffhq' in self.opts.dataset_type or 'celeb' in self.opts.dataset_type:\n                self.id_loss = id_loss.IDLoss().to(self.device).eval()\n            else:\n                self.id_loss = moco_loss.MocoLoss(opts).to(self.device).eval()\n        self.mse_loss = nn.MSELoss().to(self.device).eval()\n\n        # Initialize optimizer\n        self.optimizer = self.configure_optimizers()\n\n        # Initialize dataset\n        self.train_dataset, self.test_dataset = self.configure_datasets()\n        self.train_dataloader = DataLoader(self.train_dataset,\n                                           batch_size=self.opts.batch_size,\n                                           shuffle=True,\n                                           num_workers=int(self.opts.workers),\n                                           drop_last=True)\n        self.test_dataloader = DataLoader(self.test_dataset,\n                                          batch_size=self.opts.test_batch_size,\n                                          shuffle=False,\n                                          num_workers=int(self.opts.test_workers),\n                                          drop_last=True)\n\n        # Initialize logger\n        log_dir = os.path.join(opts.exp_dir, 'logs')\n        os.makedirs(log_dir, exist_ok=True)\n        self.logger = SummaryWriter(log_dir=log_dir)\n\n        # Initialize checkpoint dir\n        self.checkpoint_dir = os.path.join(opts.exp_dir, 'checkpoints')\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n\n\n    def train(self):\n        self.net.train()\n        while self.global_step < self.opts.max_steps:\n            for batch_idx, batch in enumerate(self.train_dataloader):\n                loss_dict = {}\n\n                x, y, y_hat, latent, res_delta, rec = self.forward(batch)\n                loss, encoder_loss_dict, id_logs = self.calc_loss(x, y, y_hat, latent, res_delta)\n                loss_dict = {**loss_dict, **encoder_loss_dict}\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n                # Logging related\n                if self.global_step % self.opts.image_interval == 0:\n                    self.parse_and_log_images(id_logs, x, y, y_hat, title='images/train/faces')\n                if self.global_step % self.opts.board_interval == 0:\n                    self.print_metrics(loss_dict, prefix='train')\n                    self.log_metrics(loss_dict, prefix='train')\n\n                # Validation related\n                val_loss_dict = None\n                if self.global_step % self.opts.val_interval == 0:\n                    val_loss_dict = self.validate()\n\n                if self.global_step % self.opts.save_interval == 0:\n                    if val_loss_dict is not None:\n                        self.checkpoint_me(val_loss_dict)\n                    else:\n                        self.checkpoint_me(loss_dict)\n\n                if self.global_step == self.opts.max_steps:\n                    break\n\n                self.global_step += 1\n\n \n    def validate(self):\n        self.net.eval()\n        agg_loss_dict = []\n        for batch_idx, batch in enumerate(self.test_dataloader):\n            cur_loss_dict = {}\n            with torch.no_grad():\n                x, y, y_hat, latent, res_delta, rec = self.forward(batch)\n                loss, cur_encoder_loss_dict, id_logs = self.calc_loss(x, y, y_hat, latent, res_delta)\n                cur_loss_dict = {**cur_loss_dict, **cur_encoder_loss_dict}\n            agg_loss_dict.append(cur_loss_dict)\n\n            # Logging related\n            self.parse_and_log_images(id_logs, x, y, y_hat,\n                                      title='images/test/faces',\n                                      subscript='{:04d}'.format(batch_idx))\n\n            # For first step just do sanity test on small amount of data\n            if self.global_step == 0 and batch_idx >= 4:\n                self.net.train()\n                return None  # Do not log, inaccurate in first batch\n\n        loss_dict = train_utils.aggregate_loss_dict(agg_loss_dict)\n        self.log_metrics(loss_dict, prefix='test')\n        self.print_metrics(loss_dict, prefix='test')\n\n        self.net.train()\n        return loss_dict\n\n    def checkpoint_me(self, loss_dict):\n        save_name =  'iteration_{}.pt'.format(self.global_step)\n        save_dict = self.__get_save_dict()\n        checkpoint_path = os.path.join(self.checkpoint_dir, save_name)\n        torch.save(save_dict, checkpoint_path)\n        with open(os.path.join(self.checkpoint_dir, 'timestamp.txt'), 'a') as f:\n            f.write('Step - {}, \\n{}\\n'.format(self.global_step, loss_dict))\n\n    def configure_optimizers(self):\n        params = list(self.net.residue.parameters())\n        params  += list(self.net.grid_align.parameters())\n        if self.opts.train_decoder:\n            params += list(self.net.decoder.parameters())\n        else:\n            self.requires_grad(self.net.decoder, False)\n            self.requires_grad(self.net.encoder, False)\n        if self.opts.optim_name == 'adam':\n            optimizer = torch.optim.Adam(params, lr=self.opts.learning_rate)\n        else:\n            optimizer = Ranger(params, lr=self.opts.learning_rate)\n        return optimizer\n\n    def configure_datasets(self):\n        if self.opts.dataset_type not in data_configs.DATASETS.keys():\n            Exception('{} is not a valid dataset_type'.format(self.opts.dataset_type))\n        print('Loading dataset for {}'.format(self.opts.dataset_type))\n        dataset_args = data_configs.DATASETS[self.opts.dataset_type]\n        transforms_dict = dataset_args['transforms'](self.opts).get_transforms()\n        train_dataset = ImagesDataset(source_root=dataset_args['train_source_root'],\n                                      target_root=dataset_args['train_target_root'],\n                                      source_transform=transforms_dict['transform_source'],\n                                      target_transform=transforms_dict['transform_gt_train'],\n                                      opts=self.opts)\n        test_dataset = ImagesDataset(source_root=dataset_args['test_source_root'],\n                                     target_root=dataset_args['test_target_root'],\n                                     source_transform=transforms_dict['transform_source'],\n                                     target_transform=transforms_dict['transform_test'],\n                                     opts=self.opts)\n        print(\"Number of training samples: {}\".format(len(train_dataset)))\n        print(\"Number of test samples: {}\".format(len(test_dataset)))\n        return train_dataset, test_dataset\n\n    def calc_loss(self, x, y, y_hat, latent, res_delta):\n        loss_dict = {}\n        loss = 0.0\n        id_logs = None\n\n        if self.opts.id_lambda > 0:  \n            loss_id, sim_improvement, id_logs = self.id_loss(y_hat, y, x)\n            loss_dict['loss_id'] = float(loss_id)\n            loss_dict['id_improve'] = float(sim_improvement)\n            loss += loss_id * self.opts.id_lambda\n        if self.opts.l2_lambda > 0:\n            loss_l2 = F.mse_loss(y_hat, y)\n            loss_dict['loss_l2'] = float(loss_l2)\n            loss += loss_l2 * self.opts.l2_lambda\n        if self.opts.lpips_lambda > 0:\n            loss_lpips = self.lpips_loss(y_hat, y)\n            loss_dict['loss_lpips'] = float(loss_lpips)\n            loss += loss_lpips * self.opts.lpips_lambda\n\n        if self.opts.res_lambda > 0:\n            target = torch.zeros_like(res_delta)\n            loss_res = F.l1_loss(res_delta, target)\n            loss_dict['loss_res'] = float(loss_res)\n            loss += loss_res * self.opts.res_lambda\n\n        loss_dict['loss'] = float(loss)\n        return loss, loss_dict, id_logs\n\n    def forward(self, batch):\n        x, y = batch\n        x, y = x.to(self.device).float(), y.to(self.device).float()\n        y_hat, latent, res_delta, rec = self.net.forward(x, return_latents=True)\n        if self.opts.dataset_type == \"cars_encode\":\n            y_hat = y_hat[:, :, 32:224, :]\n        return x, y, y_hat, latent, res_delta, rec\n\n    def log_metrics(self, metrics_dict, prefix):\n        for key, value in metrics_dict.items():\n            self.logger.add_scalar('{}/{}'.format(prefix, key), value, self.global_step)\n\n    def print_metrics(self, metrics_dict, prefix):\n        print('Metrics for {}, step {}'.format(prefix, self.global_step))\n        for key, value in metrics_dict.items():\n            print('\\t{} = '.format(key), value)\n\n    def parse_and_log_images(self, id_logs, x, y, y_hat, title, subscript=None, display_count=2):\n        im_data = []\n        for i in range(display_count):\n            cur_im_data = {\n                'input_face': common.log_input_image(x[i], self.opts),\n                'target_face': common.tensor2im(y[i]),\n                'output_face': common.tensor2im(y_hat[i]),\n            }\n            if id_logs is not None:\n                for key in id_logs[i]:\n                    cur_im_data[key] = id_logs[i][key]\n            im_data.append(cur_im_data)\n        self.log_images(title, im_data=im_data, subscript=subscript)\n\n    def log_images(self, name, im_data, subscript=None, log_latest=False):\n        fig = common.vis_faces(im_data)\n        step = self.global_step\n        if log_latest:\n            step = 0\n        if subscript:\n            path = os.path.join(self.logger.log_dir, name, '{}_{:04d}.jpg'.format(subscript, step))\n        else:\n            path = os.path.join(self.logger.log_dir, name, '{:04d}.jpg'.format(step))\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        fig.savefig(path)\n        plt.close(fig)\n\n    def __get_save_dict(self):\n        save_dict = {\n            'state_dict': self.net.state_dict(),\n            'opts': vars(self.opts)}\n\n        if self.opts.start_from_latent_avg:\n            save_dict['latent_avg'] = self.net.latent_avg\n        return save_dict\n\n    @staticmethod\n    def requires_grad(model, flag=True):\n        for p in model.parameters():\n            p.requires_grad = flag","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:06:19.789806Z","iopub.execute_input":"2024-04-22T22:06:19.790231Z","iopub.status.idle":"2024-04-22T22:06:19.804178Z","shell.execute_reply.started":"2024-04-22T22:06:19.790198Z","shell.execute_reply":"2024-04-22T22:06:19.803297Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/HFGI/training/coach.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile train.sh\npython ./scripts/train.py   --dataset_type='ffhq_encode'  --start_from_latent_avg \\\n--id_lambda=0.1  --val_interval=10000 --save_interval=5000 --max_steps=100000  --stylegan_size=1024 --is_train=True \\\n--distortion_scale=0.15 --aug_rate=0.9 --res_lambda=0.1  \\\n--stylegan_weights='./pretrained/stylegan2-ffhq-config-f.pt' --checkpoint_path='/kaggle/input/model_30k/pytorch/rgb-30k/1/model_rgb_loss.pt'  \\\n--workers=4  --batch_size=8  --test_batch_size=8 --test_workers=4 --exp_dir='./experiment/run2' --image_interval=100\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:06:32.469280Z","iopub.execute_input":"2024-04-22T22:06:32.470062Z","iopub.status.idle":"2024-04-22T22:06:32.476122Z","shell.execute_reply.started":"2024-04-22T22:06:32.470031Z","shell.execute_reply":"2024-04-22T22:06:32.475089Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Overwriting train.sh\n","output_type":"stream"}]},{"cell_type":"code","source":"!bash train.sh","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:06:33.582263Z","iopub.execute_input":"2024-04-22T22:06:33.582622Z","iopub.status.idle":"2024-04-22T22:15:19.146743Z","shell.execute_reply.started":"2024-04-22T22:06:33.582592Z","shell.execute_reply":"2024-04-22T22:15:19.145454Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"2024-04-22 22:06:37.594304: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-22 22:06:37.594369: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-22 22:06:37.595935: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n{'aug_rate': 0.9,\n 'batch_size': 8,\n 'board_interval': 100,\n 'checkpoint_path': '/kaggle/input/model_30k/pytorch/rgb-30k/1/model_rgb_loss.pt',\n 'dataset_type': 'ffhq_encode',\n 'discriminator_lambda': 0,\n 'discriminator_lr': 2e-05,\n 'distortion_scale': 0.15,\n 'encoder_type': 'Encoder4Editing',\n 'exp_dir': './experiment/run2',\n 'id_lambda': 0.1,\n 'image_interval': 100,\n 'is_train': True,\n 'l2_lambda': 1.0,\n 'learning_rate': 0.0001,\n 'lpips_lambda': 0.8,\n 'lpips_type': 'alex',\n 'max_steps': 100000,\n 'optim_name': 'ranger',\n 'res_lambda': 0.1,\n 'save_interval': 5000,\n 'start_from_latent_avg': True,\n 'stylegan_size': 1024,\n 'stylegan_weights': './pretrained/stylegan2-ffhq-config-f.pt',\n 'test_batch_size': 8,\n 'test_workers': 4,\n 'train_decoder': False,\n 'val_interval': 10000,\n 'workers': 4}\nLoading basic encoder from checkpoint: /kaggle/input/model_30k/pytorch/rgb-30k/1/model_rgb_loss.pt\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nLoading ResNet ArcFace\nLoading dataset for ffhq_encode\nNumber of training samples: 30000\nNumber of test samples: 12\n/kaggle/working/HFGI/./training/ranger.py:123: UserWarning: This overload of addcmul_ is deprecated:\n\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\nConsider using one of the following signatures instead:\n\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/python_arg_parser.cpp:1519.)\n  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\nMetrics for train, step 0\n\tloss_id =  0.9642136096954346\n\tid_improve =  -0.9454139858717099\n\tloss_l2 =  0.3052590787410736\n\tloss_lpips =  0.6184893250465393\n\tloss_res =  1.4102739095687866\n\tloss =  1.0374993085861206\nMetrics for test, step 0\n\tloss_id =  0.6913577318191528\n\tid_improve =  -0.6679826565086842\n\tloss_l2 =  0.36132845282554626\n\tloss_lpips =  0.44332149624824524\n\tloss_res =  0.46843475103378296\n\tloss =  0.831964910030365\nMetrics for train, step 100\n\tloss_id =  0.8650292158126831\n\tid_improve =  -0.8530029468238354\n\tloss_l2 =  0.1360037922859192\n\tloss_lpips =  0.5308590531349182\n\tloss_res =  1.2063028812408447\n\tloss =  0.7678242325782776\nMetrics for train, step 200\n\tloss_id =  0.7753682136535645\n\tid_improve =  -0.7575619912240654\n\tloss_l2 =  0.09872769564390182\n\tloss_lpips =  0.4128563404083252\n\tloss_res =  0.9288482069969177\n\tloss =  0.5994344353675842\nMetrics for train, step 300\n\tloss_id =  0.7982997298240662\n\tid_improve =  -0.7788913827389479\n\tloss_l2 =  0.12484379857778549\n\tloss_lpips =  0.3871176540851593\n\tloss_res =  0.8197709918022156\n\tloss =  0.596345067024231\n^C\nTraceback (most recent call last):\n  File \"/kaggle/working/HFGI/./scripts/train.py\", line 36, in <module>\n    main()\n  File \"/kaggle/working/HFGI/./scripts/train.py\", line 21, in main\n    coach.train()\n  File \"/kaggle/working/HFGI/./training/coach.py\", line 81, in train\n    self.optimizer.step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 373, in wrapper\n    out = func(*args, **kwargs)\n  File \"/kaggle/working/HFGI/./training/ranger.py\", line 161, in step\n    slow_p.add_(self.alpha, p.data - slow_p)  # (fast weights - slow weights) * alpha\nKeyboardInterrupt\n","output_type":"stream"}]}]}