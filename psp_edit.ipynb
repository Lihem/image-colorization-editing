{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.stylegan2.model import Generator  # Assuming this is the path to the Generator class\n",
    "import numpy as np\n",
    "from utils import common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = '/home/melih.cosgun/GENERATIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (style): Sequential(\n",
       "    (0): PixelNorm()\n",
       "    (1): EqualLinear(512, 512)\n",
       "    (2): EqualLinear(512, 512)\n",
       "    (3): EqualLinear(512, 512)\n",
       "    (4): EqualLinear(512, 512)\n",
       "    (5): EqualLinear(512, 512)\n",
       "    (6): EqualLinear(512, 512)\n",
       "    (7): EqualLinear(512, 512)\n",
       "    (8): EqualLinear(512, 512)\n",
       "  )\n",
       "  (input): ConstantInput()\n",
       "  (conv1): StyledConv(\n",
       "    (conv): ModulatedConv2d(512, 512, 3, upsample=False, downsample=False)\n",
       "    (noise): NoiseInjection()\n",
       "    (activate): FusedLeakyReLU()\n",
       "  )\n",
       "  (to_rgb1): ToRGB(\n",
       "    (conv): ModulatedConv2d(512, 3, 1, upsample=False, downsample=False)\n",
       "  )\n",
       "  (convs): ModuleList(\n",
       "    (0): StyledConv(\n",
       "      (conv): ModulatedConv2d(512, 512, 3, upsample=True, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (1): StyledConv(\n",
       "      (conv): ModulatedConv2d(512, 512, 3, upsample=False, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (2): StyledConv(\n",
       "      (conv): ModulatedConv2d(512, 512, 3, upsample=True, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (3): StyledConv(\n",
       "      (conv): ModulatedConv2d(512, 512, 3, upsample=False, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (4): StyledConv(\n",
       "      (conv): ModulatedConv2d(512, 512, 3, upsample=True, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (5): StyledConv(\n",
       "      (conv): ModulatedConv2d(512, 512, 3, upsample=False, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (6): StyledConv(\n",
       "      (conv): ModulatedConv2d(512, 512, 3, upsample=True, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (7): StyledConv(\n",
       "      (conv): ModulatedConv2d(512, 512, 3, upsample=False, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (8): StyledConv(\n",
       "      (conv): ModulatedConv2d(512, 256, 3, upsample=True, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (9): StyledConv(\n",
       "      (conv): ModulatedConv2d(256, 256, 3, upsample=False, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (10): StyledConv(\n",
       "      (conv): ModulatedConv2d(256, 128, 3, upsample=True, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (11): StyledConv(\n",
       "      (conv): ModulatedConv2d(128, 128, 3, upsample=False, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (12): StyledConv(\n",
       "      (conv): ModulatedConv2d(128, 64, 3, upsample=True, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (13): StyledConv(\n",
       "      (conv): ModulatedConv2d(64, 64, 3, upsample=False, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (14): StyledConv(\n",
       "      (conv): ModulatedConv2d(64, 32, 3, upsample=True, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (15): StyledConv(\n",
       "      (conv): ModulatedConv2d(32, 32, 3, upsample=False, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "  )\n",
       "  (upsamples): ModuleList()\n",
       "  (to_rgbs): ModuleList(\n",
       "    (0-3): 4 x ToRGB(\n",
       "      (upsample): Upsample()\n",
       "      (conv): ModulatedConv2d(512, 3, 1, upsample=False, downsample=False)\n",
       "    )\n",
       "    (4): ToRGB(\n",
       "      (upsample): Upsample()\n",
       "      (conv): ModulatedConv2d(256, 3, 1, upsample=False, downsample=False)\n",
       "    )\n",
       "    (5): ToRGB(\n",
       "      (upsample): Upsample()\n",
       "      (conv): ModulatedConv2d(128, 3, 1, upsample=False, downsample=False)\n",
       "    )\n",
       "    (6): ToRGB(\n",
       "      (upsample): Upsample()\n",
       "      (conv): ModulatedConv2d(64, 3, 1, upsample=False, downsample=False)\n",
       "    )\n",
       "    (7): ToRGB(\n",
       "      (upsample): Upsample()\n",
       "      (conv): ModulatedConv2d(32, 3, 1, upsample=False, downsample=False)\n",
       "    )\n",
       "  )\n",
       "  (noises): Module()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "device = \"cuda:1\" \n",
    "stylegan_model_path = \"pretrained_models/stylegan2-ffhq-config-f.pt\" \n",
    "\n",
    "stylegan_model = Generator(1024, 512, 8).to(device)\n",
    "\n",
    "state_dict = torch.load(stylegan_model_path)\n",
    "face_pool = torch.nn.AdaptiveAvgPool2d((256, 256))\n",
    "stylegan_model.load_state_dict(state_dict['g_ema'], strict=False)\n",
    "\n",
    "stylegan_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Generation\n",
    "\n",
    "latent_z = np.random.randn(1, 512).astype(\"float32\")\n",
    "latent_z = torch.from_numpy(latent_z).to(device)\n",
    "\n",
    "img, w_plus = stylegan_model(styles=[latent_z], input_is_latent=False, randomize_noise=False, return_latents=True)   # Adapt this line according to the actual function\n",
    "\n",
    "img = face_pool(img)\n",
    "\n",
    "img = common.tensor2im(img[0])\n",
    "\n",
    "resize_amount = (256, 256)\n",
    "img.resize(resize_amount)\n",
    "\n",
    "img.save(f'{project_path}/test_images/test.png')\n",
    "\n",
    "w_plus = w_plus[0].detach().cpu().numpy()\n",
    "\n",
    "# Save the latent code\n",
    "np.save(f'{project_path}/test_latents/test.npy' , w_plus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "EXPERIMENT_DATA_ARGS = {\n",
    "    \"ffhq_encode\": {\n",
    "        \"model_path\": \"pretrained_models/model_rgb_loss.pt\", # changed to custom model\n",
    "        \"image_path\": \"/home/melih.cosgun/GENERATIVE/test_inputs/input_img.png\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    }\n",
    "}\n",
    "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS['ffhq_encode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pSp from checkpoint: pretrained_models/model_rgb_loss.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GradualStyleEncoder:\n\tMissing key(s) in state_dict: \"condition_layer.conv1.weight\", \"condition_layer.conv1.bias\", \"condition_layer.conv2.weight\", \"condition_layer.conv2.bias\", \"condition_layer.conv3.weight\", \"condition_layer.conv3.bias\", \"condition_layer.bn1.weight\", \"condition_layer.bn1.bias\", \"condition_layer.bn1.running_mean\", \"condition_layer.bn1.running_var\", \"condition_layer.bn2.weight\", \"condition_layer.bn2.bias\", \"condition_layer.bn2.running_mean\", \"condition_layer.bn2.running_var\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m     17\u001b[0m opts \u001b[38;5;241m=\u001b[39m Namespace(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n\u001b[0;32m---> 18\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mpSp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     21\u001b[0m net \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/GENERATIVE/pixel2style2pixel/models/psp.py:34\u001b[0m, in \u001b[0;36mpSp.__init__\u001b[0;34m(self, opts)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_pool \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool2d((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Load weights if needed\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GENERATIVE/pixel2style2pixel/models/psp.py:51\u001b[0m, in \u001b[0;36mpSp.load_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading pSp from checkpoint: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopts\u001b[38;5;241m.\u001b[39mcheckpoint_path))\n\u001b[1;32m     50\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopts\u001b[38;5;241m.\u001b[39mcheckpoint_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mload_state_dict(get_keys(ckpt, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m'\u001b[39m), strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load_latent_avg(ckpt)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env_copy/lib/python3.9/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GradualStyleEncoder:\n\tMissing key(s) in state_dict: \"condition_layer.conv1.weight\", \"condition_layer.conv1.bias\", \"condition_layer.conv2.weight\", \"condition_layer.conv2.bias\", \"condition_layer.conv3.weight\", \"condition_layer.conv3.bias\", \"condition_layer.bn1.weight\", \"condition_layer.bn1.bias\", \"condition_layer.bn1.running_mean\", \"condition_layer.bn1.running_var\", \"condition_layer.bn2.weight\", \"condition_layer.bn2.bias\", \"condition_layer.bn2.running_mean\", \"condition_layer.bn2.running_var\". "
     ]
    }
   ],
   "source": [
    "from models.psp import pSp\n",
    "from argparse import Namespace\n",
    "\n",
    "model_path = EXPERIMENT_ARGS['model_path']\n",
    "ckpt = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "opts = ckpt['opts']\n",
    "\n",
    "opts['device'] = device\n",
    "\n",
    "opts['checkpoint_path'] = model_path\n",
    "if 'learn_in_w' not in opts:\n",
    "    opts['learn_in_w'] = False\n",
    "if 'output_size' not in opts:\n",
    "    opts['output_size'] = 1024\n",
    "\n",
    "opts = Namespace(**opts)\n",
    "net = pSp(opts)\n",
    "net.eval()\n",
    "\n",
    "net = net.to(device)\n",
    "#net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "from scripts.align_all_parallel import align_face\n",
    "def run_alignment(image_path):\n",
    "  predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "  aligned_image = align_face(filepath=image_path, predictor=predictor)\n",
    "  print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
    "  return aligned_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned image has shape: (256, 256)\n"
     ]
    }
   ],
   "source": [
    "image_path = f'{project_path}/test_inputs/input_img.png'\n",
    "\n",
    "input_image = run_alignment(image_path)\n",
    "\n",
    "input_image.resize((256, 256))\n",
    "\n",
    "img_transforms = EXPERIMENT_ARGS['transform']\n",
    "\n",
    "transformed_image = img_transforms(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input = transformed_image.unsqueeze(0)\n",
    "    result_image, w_plus = net(input.to(device).float(), randomize_noise=False, return_latents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = common.tensor2im(result_image[0])\n",
    "output_image.save(f'{project_path}/test_images/test.png')\n",
    "w_plus = w_plus[0].detach().cpu().numpy()\n",
    "np.save(f'{project_path}/test_latents/test.npy' , w_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_latents = np.load(f'{project_path}/test_latents/test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 512)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for modification : Bald\n",
      "Running for modification : Black_Hair\n",
      "Running for modification : Blond_Hair\n",
      "Running for modification : Eyeglasses\n",
      "Running for modification : Goatee\n",
      "Running for modification : Male\n",
      "Running for modification : Mustache\n",
      "Running for modification : Smiling\n"
     ]
    }
   ],
   "source": [
    "modification_list = ['Bald', 'Black_Hair', 'Blond_Hair', 'Eyeglasses', 'Goatee', 'Male', 'Mustache', 'Smiling']\n",
    "\n",
    "for modification in modification_list:\n",
    "    print(f'Running for modification : {modification}')\n",
    "    modifying_latent = np.load(f'{project_path}/svm/1.0/{modification}.npy')\n",
    "    modification_experiment_path = os.path.join(f'{project_path}/output/', modification)\n",
    "    os.makedirs(modification_experiment_path, exist_ok=True)\n",
    "\n",
    "    modifying_latent = modifying_latent.reshape(18, 512)\n",
    "    modifying_latent = modifying_latent.astype(np.float32)\n",
    "\n",
    "    for i in range(0, 50, 5):\n",
    "        edited_latents = original_latents + i * modifying_latent\n",
    "        edited_latents = torch.from_numpy(edited_latents).to(device)\n",
    "#img, _ = stylegan_model(styles=[edited_latents[0].unsqueeze(0)], input_is_latent=True, randomize_noise=False, return_latents=False) \n",
    "        img, _ = stylegan_model(styles=[edited_latents.unsqueeze(0)], input_is_latent=True, randomize_noise=False, return_latents=False) \n",
    "\n",
    "        img = face_pool(img)\n",
    "\n",
    "        img = common.tensor2im(img[0])\n",
    "\n",
    "        resize_amount = (256, 256)\n",
    "        img.resize(resize_amount)\n",
    "\n",
    "        img.save(os.path.join(modification_experiment_path, f'{i}.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env_copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
